{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: PCA vs. Kernel PCA on Non-linear Data\n",
    "\n",
    "## Objective\n",
    "Compare linear Principal Component Analysis (PCA) with Kernel PCA on a non-linear dataset, focusing on how each method captures and reconstructs the data's structure.\n",
    "\n",
    "## Dataset\n",
    "We'll use the synthetic \"moons\" dataset from sklearn.datasets.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Data Generation:\n",
    "   - Generate the \"moons\" dataset using sklearn.datasets.make_moons.\n",
    "   - Add Gaussian noise to the dataset.\n",
    "\n",
    "2. Implement PCA:\n",
    "   - Apply standard PCA to reduce the dataset to 1 dimension.\n",
    "   - Use the inverse_transform method to project the data back to 2 dimensions.\n",
    "   - Visualize the original 2D data and the PCA reconstruction.\n",
    "\n",
    "3. Implement Kernel PCA:\n",
    "   - Apply Kernel PCA with a radial basis function (RBF) kernel to reduce the dataset to 1 dimension.\n",
    "   - Use the inverse_transform method to project the data back to 2 dimensions.\n",
    "   - Visualize the original 2D data and the Kernel PCA reconstruction.\n",
    "\n",
    "4. Classification Task:\n",
    "   - Split the data into training and testing sets.\n",
    "   - Train a simple classifier (e.g., Logistic Regression) on:\n",
    "     a) The original 2D data\n",
    "     b) The 2D PCA reconstruction\n",
    "     c) The 2D Kernel PCA reconstruction\n",
    "   - Compare the classification accuracies.\n",
    "\n",
    "5. Parameter Exploration (Optional):\n",
    "   - Experiment with different noise levels in the dataset.\n",
    "   - Try different kernels for Kernel PCA (e.g., polynomial, sigmoid).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to generate noisy moons dataset\n",
    "def generate_noisy_moons(n_samples=1000, noise=0.1):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_noisy_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "# Apply PCA\n",
    "pca = #Code here\n",
    "\n",
    "# Apply Kernel PCA\n",
    "kpca = #Code here\n",
    "\n",
    "\n",
    "# Visualize the data and reconstructions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.title('Original Data')\n",
    "\n",
    "# PCA reconstruction\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_pca_reconstructed[:, 0], X_pca_reconstructed[:, 1], c=y, cmap='viridis')\n",
    "plt.title('PCA Reconstruction')\n",
    "\n",
    "# Kernel PCA reconstruction\n",
    "plt.subplot(133)\n",
    "plt.scatter(X_kpca_reconstructed[:, 0], X_kpca_reconstructed[:, 1], c=y, cmap='viridis')\n",
    "plt.title('Kernel PCA Reconstruction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification task\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Original data\n",
    "clf_original = LogisticRegression(random_state=42)\n",
    "clf_original.fit(X_train, y_train)\n",
    "y_pred_original = clf_original.predict(X_test)\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# PCA\n",
    "# RUN PCA HERE AND CALCULATE ACCURACY\n",
    "\n",
    "# Kernel PCA\n",
    "# RUN Kernel PCA HERE AND CALCULATE ACCURACY\n",
    "\n",
    "print(f\"Accuracy on original data: {acc_original:.4f}\")\n",
    "print(f\"Accuracy with PCA reconstruction: {acc_pca:.4f}\")\n",
    "print(f\"Accuracy with Kernel PCA reconstruction: {acc_kpca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Comparison of Manifold Learning Techniques\n",
    "\n",
    "## Objective\n",
    "Compare different manifold learning techniques on a synthetic dataset to understand their strengths and weaknesses in capturing non-linear structures and reducing dimensionality.\n",
    "\n",
    "## Dataset\n",
    "We'll use the \"S-curve\" dataset, which can be generated using sklearn.datasets.make_s_curve.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Data Generation:\n",
    "   - Examine the data generation code, and understand what it is doing\n",
    "\n",
    "2. Implement and Visualize Different Manifold Learning Techniques:\n",
    "   Apply the following techniques to reduce the 3D S-curve to 2D:\n",
    "   a) PCA (as a baseline linear method)\n",
    "   b) Isomap\n",
    "   c) Locally Linear Embedding (LLE)\n",
    "   d) t-SNE\n",
    "   e) UMAP\n",
    "   Visualize the results of each method.\n",
    "\n",
    "3. Quantitative Comparison:\n",
    "   - Implement a function to calculate the trustworthiness (from sklearn.manifold) of each embedding.\n",
    "   - Compare the trustworthiness scores for each method.\n",
    "\n",
    "4. Computational Efficiency:\n",
    "   - Measure and compare the execution time of each method.\n",
    "\n",
    "5. Parameter Exploration:\n",
    "   - For one of the methods (e.g., t-SNE or UMAP), explore how changing a key parameter affects the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, TSNE\n",
    "from sklearn.manifold import trustworthiness\n",
    "import umap\n",
    "import time\n",
    "\n",
    "\n",
    "def make_double_s_curve(n_samples=1000, noise=0.1, random_state=None):\n",
    "    \"\"\"Generate a dataset with two intertwined S-curves\"\"\"\n",
    "    X1, _ = make_s_curve(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "    X2, _ = make_s_curve(n_samples=n_samples, noise=noise, random_state=random_state+1)\n",
    "    \n",
    "    # Rotate the second S-curve by swapping axes\n",
    "    X2 = X2[:, [1, 2, 0]]\n",
    "    \n",
    "    # Combine the two S-curves\n",
    "    X = X1 * X2\n",
    "    \n",
    "    return X\n",
    "\n",
    "def create_rgb_colors(X):\n",
    "    \"\"\"Create RGB colors based on the 3D coordinates\"\"\"\n",
    "    # Normalize X to [0, 1] range\n",
    "    X_norm = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    \n",
    "    # Scale to [0.2, 0.7] range\n",
    "    X_color = X_norm * 0.5 + 0.2\n",
    "    \n",
    "    return X_color\n",
    "\n",
    "# Generate the dataset\n",
    "n_points = 1000\n",
    "X = make_double_s_curve(n_samples=n_points, noise=0.05, random_state=42)\n",
    "\n",
    "# Create RGB colors\n",
    "colors = create_rgb_colors(X)\n",
    "\n",
    "\n",
    "# Add some noise\n",
    "noise = np.random.normal(scale=0.1, size=X.shape)\n",
    "X_noisy = X + noise\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_embedding(ax, X, title):\n",
    "    x_min, x_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# List of manifold learning methods\n",
    "methods = [\n",
    "    # TODO: Fill out the code here\n",
    "]\n",
    "\n",
    "# TODO: Apply each method and measure time\n",
    "# Hint - create a method that iterates over the \"methods\" array above\n",
    "results = []\n",
    "times = []\n",
    "trustworthiness_scores = []\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "# Plot original data\n",
    "ax = fig.add_subplot(231, projection='3d')\n",
    "ax.scatter(X_noisy[:, 0], X_noisy[:, 1], X_noisy[:, 2], c=colors)\n",
    "ax.set_title(\"Original S-curve\")\n",
    "\n",
    "# Create trustworthiness scores\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Method\\t\\tTime (s)\\tTrustworthiness\")\n",
    "print(\"-\" * 50)\n",
    "for (name, _), t, trust in zip(methods, times, trustworthiness_scores):\n",
    "    print(f\"{name:<10}\\t{t:.2f}\\t\\t{trust:.4f}\")\n",
    "\n",
    "# Parameter exploration for t-SNE\n",
    "perplexities = [5, 30, 50, 100]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "# And in the t-SNE parameter exploration:\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=perplexity)\n",
    "    X_tsne = tsne.fit_transform(X_noisy)\n",
    "    ax = axs[i]\n",
    "    plot_embedding(ax,X_tsne, f\"t-SNE (perplexity={perplexity})\")  # Pass color here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Feature Selection vs. Dimensionality Reduction\n",
    "\n",
    "## Objective\n",
    "Compare feature selection methods with dimensionality reduction techniques, understanding their differences, strengths, and weaknesses in the context of a real-world dataset.\n",
    "\n",
    "## Dataset\n",
    "We'll use the Wine Quality dataset from the UCI Machine Learning Repository. This dataset contains various chemical properties of wines and their quality ratings.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Dimensionality Reduction Techniques:\n",
    "\n",
    "1. Principal Component Analysis (PCA):\n",
    "   - PCA transforms the original features into a new set of uncorrelated features (principal components).\n",
    "   - These components are ordered by the amount of variance they explain in the data.\n",
    "   - PCA is useful for reducing dimensionality while retaining as much variance as possible.\n",
    "\n",
    "2. Uniform Manifold Approximation and Projection (UMAP):\n",
    "   - UMAP is a non-linear dimensionality reduction technique.\n",
    "   - It tries to preserve both local and global structure of the data in the lower-dimensional space.\n",
    "   - UMAP is particularly good for visualization and can often capture more complex relationships than PCA.\n",
    "\n",
    "### Feature Selection Methods:\n",
    "\n",
    "1. Recursive Feature Elimination (RFE):\n",
    "   - RFE recursively removes features, building a model (e.g., Random Forest) each time.\n",
    "   - It ranks features by importance, recursively eliminating the least important features.\n",
    "   - RFE helps select a subset of the original features that are most relevant to the target variable.\n",
    "\n",
    "2. Lasso (Least Absolute Shrinkage and Selection Operator):\n",
    "   - Lasso is a linear regression method that includes L1 regularization.\n",
    "   - It tends to produce sparse models by shrinking some coefficients to exactly zero.\n",
    "   - Features with non-zero coefficients are considered selected.\n",
    "   - Lasso is useful for feature selection in linear models and can handle multicollinearity.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Data Loading and Preprocessing:\n",
    "   - Load the Wine Quality dataset.\n",
    "   - Perform basic preprocessing (handling missing values if any, scaling features).\n",
    "   - Split the data into features (X) and target variable (y).\n",
    "\n",
    "2. Implement PCA:\n",
    "   - Apply PCA to reduce the dimensionality of the dataset.\n",
    "   - Determine the number of components needed to explain 95% of the variance.\n",
    "\n",
    "3. Implement UMAP:\n",
    "   - Apply UMAP to reduce the dimensionality of the dataset to 2 components.\n",
    "   - Visualize the UMAP embedding, coloring points by wine quality.\n",
    "\n",
    "4. Feature Selection Methods:\n",
    "   - Implement Recursive Feature Elimination (RFE) with Random Forest as the estimator.\n",
    "   - Implement Lasso for feature selection.\n",
    "\n",
    "5. Model Training and Evaluation:\n",
    "   - Use a Random Forest Classifier for this task.\n",
    "   - Train and evaluate the model using:\n",
    "     a) All original features\n",
    "     b) PCA-reduced features\n",
    "     c) UMAP-reduced features\n",
    "     d) RFE-selected features\n",
    "     e) Lasso-selected features\n",
    "   - Compare the performance (accuracy and training time) of each method.\n",
    "\n",
    "6. Feature Importance Analysis:\n",
    "   - For RFE and Lasso, identify the top selected features.\n",
    "   - For the original Random Forest model, plot feature importances.\n",
    "   - Compare the selected/important features across methods.\n",
    "\n",
    "7. Analysis:\n",
    "   - Discuss the trade-offs between feature selection and dimensionality reduction.\n",
    "   - Analyze how each method affects model performance and interpretability.\n",
    "   - Consider scenarios where each approach (feature selection vs. dimensionality reduction) might be more appropriate.\n",
    "\n",
    "## Hints\n",
    "- Use pandas for data loading and preprocessing.\n",
    "- sklearn provides implementations for PCA, RFE, Lasso, and Random Forest.\n",
    "- Use umap-learn library for UMAP.\n",
    "- matplotlib and seaborn will be useful for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import umap\n",
    "import time\n",
    "\n",
    "# Load the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Preprocessing\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA\n",
    "# TODO: Run PCA to get t .95 variance\n",
    "\n",
    "\n",
    "# UMAP\n",
    "# TODO: Run Umap with 2 components\n",
    "\n",
    "\n",
    "# Visualize UMAP\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_train, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP projection of Wine Quality Data')\n",
    "plt.show()\n",
    "\n",
    "# RFE\n",
    "# TODO Use RFE with RandomForestClassifier\n",
    "\n",
    "# Lasso\n",
    "# TODO Use Lasso; select columns with coeff > 0\n",
    "\n",
    "\n",
    "# Model training and evaluation\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, method_name):\n",
    "    #TODO: Run a random forest classifer\n",
    "    # Should just print time and accuracy\n",
    "    # Can return accuracy if you want.\n",
    "\n",
    "results = {}\n",
    "results['Original'] = train_and_evaluate(X_train_scaled, X_test_scaled, y_train, y_test, \"Original\")\n",
    "results['PCA'] = train_and_evaluate(X_train_pca, X_test_pca, y_train, y_test, \"PCA\")\n",
    "results['UMAP'] = train_and_evaluate(X_train_umap, X_test_umap, y_train, y_test, \"UMAP\")\n",
    "results['RFE'] = train_and_evaluate(X_train_rfe, X_test_rfe, y_train, y_test, \"RFE\")\n",
    "results['Lasso'] = train_and_evaluate(X_train_lasso, X_test_lasso, y_train, y_test, \"Lasso\")\n",
    "\n",
    "# Feature importance analysis\n",
    "rf_original = results['Original'][0]\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf_original.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importances (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop features selected by RFE:\")\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "print(\"\\nFeatures selected by Lasso:\")\n",
    "print(lasso_selected_features)\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"1. Dimensionality reduction (PCA, UMAP) transforms the original features into a new space.\")\n",
    "print(\"2. Feature selection (RFE, Lasso) chooses a subset of the original features.\")\n",
    "print(\"3. Compare the accuracy and training time for each method.\")\n",
    "print(\"4. Notice how feature selection methods provide more interpretable results.\")\n",
    "print(\"5. Consider the trade-off between model performance, training time, and interpretability.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
