{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab Exercises\n",
    "\n",
    "## Exercise 1: Understanding MLPs and Network Architecture\n",
    "\n",
    "1. Complete the code to train each architecture for 50 epochs\n",
    "2. Record training and validation accuracy for each\n",
    "3. Plot the learning curves\n",
    "4. Explain which architecture performed best and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create and evaluate model\n",
    "def create_model(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_sizes[0], activation='relu', input_shape=(4,)))\n",
    "    for size in layer_sizes[1:-1]:\n",
    "        model.add(Dense(size, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Try different architectures\n",
    "architectures = [\n",
    "    [10, 3],\n",
    "    [20, 10, 3],\n",
    "    [30, 20, 10, 3]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2: Impact of Batch Size and Learning Rate\n",
    "Using the best architecture from Exercise 1, explore how batch size and learning rate affect training:\n",
    "\n",
    "Tasks:\n",
    "1. Create a grid of experiments testing different batch sizes and learning rates\n",
    "2. Plot training curves for each combination\n",
    "3. Analyze how these parameters affect:\n",
    "   - Training speed\n",
    "   - Final accuracy\n",
    "   - Stability of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test different combinations\n",
    "batch_sizes = [8, 32, 64]\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_model(batch_size, lr):\n",
    "    model = create_model([20, 10, 3])  # Use best architecture from Ex 1\n",
    "    model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=50,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 3: Comparing MLPs with Traditional Models\n",
    "\n",
    "1. Complete the MLP comparison code\n",
    "2. Run comparisons on both datasets\n",
    "3. Explain why each model performs better or worse on each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset 1: Wisconsin Breast Cancer dataset (good for traditional ML)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_c, y_c = cancer.data, cancer.target\n",
    "\n",
    "# Dataset 2: Fashion MNIST subset (good for neural networks)\n",
    "# Only use 1000 samples to keep training time reasonable\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = fashion_mnist.load_data()\n",
    "X_f = X_train_full[:1000].reshape(1000, -1) / 255.0\n",
    "y_f = y_train_full[:1000]\n",
    "\n",
    "# Function to compare models\n",
    "def compare_models(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    rf_score = rf.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # MLP\n",
    "    # Complete this code\n",
    "    \n",
    "    return rf_score, mlp_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 4: Early Stopping and Overfitting (15 minutes)\n",
    "\n",
    "1. Train the model without early stopping for 200 epochs\n",
    "2. Implement early stopping with appropriate parameters\n",
    "3. Compare training curves and final test performance\n",
    "4. Visualize decision boundaries for both models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate a small noisy dataset prone to overfitting\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create an intentionally complex model\n",
    "def create_complex_model():\n",
    "    model = Sequential([\n",
    "        Dense(100, activation='relu', input_shape=(2,)),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
