{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Comparing Network Architectures with the Boston Housing Dataset\n",
    "\n",
    "In this exercise, you'll implement and compare three different neural network architectures using the Boston Housing dataset:\n",
    "1. A basic sequential network\n",
    "2. A wide & deep network\n",
    "3. A wide & deep network with auxiliary output\n",
    "\n",
    "The goal is to understand how different architectures affect model performance and training dynamics.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import our dependencies and prepare our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "(X_full, y_full), _ = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# Split the data\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Sequential Model\n",
    "\n",
    "Implement a basic sequential model with three dense layers. Use ReLU activation for hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequential_model():\n",
    "    # TODO: Implement a sequential model with:\n",
    "    # - Input layer (13 features)\n",
    "    # - Hidden layer with 30 neurons and ReLU activation\n",
    "    # - Hidden layer with 30 neurons and ReLU activation\n",
    "    # - Output layer with 1 neuron (no activation)\n",
    "    # Use \"he_normal\" initialization for the layers\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model_sequential = create_sequential_model()\n",
    "model_sequential.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"RootMeanSquaredError\"])\n",
    "history_sequential = model_sequential.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    epochs=20,\n",
    "    validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "plot_learning_curves(history_sequential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Wide & Deep Model\n",
    "\n",
    "Create a wide & deep model using the Functional API. Split the input features into two groups:\n",
    "- Wide path: Features 0-5 (size-related features)\n",
    "- Deep path: Features 6-12 (location and age-related features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_wide_and_deep_model():\n",
    "    # TODO: Implement a wide & deep model with:\n",
    "    # - Separate inputs for wide and deep paths\n",
    "    # - Deep path: two dense layers with 30 neurons each\n",
    "    # - Concatenate wide path with deep path output\n",
    "    # - Final dense layer with 1 neuron\n",
    "    # Use normalization layers for both paths\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model_wide_deep = create_wide_and_deep_model()\n",
    "model_wide_deep.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "# TODO: Prepare the wide and deep inputs\n",
    "X_train_wide = # Your code here\n",
    "X_train_deep = # Your code here\n",
    "X_valid_wide = # Your code here\n",
    "X_valid_deep = # Your code here\n",
    "\n",
    "history_wide_deep = model_wide_deep.fit(\n",
    "    [X_train_wide, X_train_deep], y_train,\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_wide, X_valid_deep], y_valid))\n",
    "\n",
    "plot_learning_curves(history_wide_deep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Wide & Deep Model with Auxiliary Output\n",
    "\n",
    "Enhance the wide & deep model by adding an auxiliary output from the deep path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_wide_and_deep_aux_model():\n",
    "    # TODO: Implement a wide & deep model with auxiliary output:\n",
    "    # - Same structure as previous wide & deep model\n",
    "    # - Add auxiliary output from the second dense layer of deep path\n",
    "    # - Main output from concatenated paths\n",
    "    # Use normalization layers for both paths\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model_wide_deep_aux = create_wide_and_deep_aux_model()\n",
    "model_wide_deep_aux.compile(\n",
    "    loss=[\"mse\", \"mse\"],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "history_wide_deep_aux = model_wide_deep_aux.fit(\n",
    "    [X_train_wide, X_train_deep], [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_wide, X_valid_deep], [y_valid, y_valid]))\n",
    "\n",
    "plot_learning_curves(history_wide_deep_aux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Analysis\n",
    "\n",
    "1. Compare the training curves for all three models. Which architecture learns fastest?\n",
    "2. Calculate and compare the test set performance for each model.\n",
    "3. What are the advantages and disadvantages of each architecture based on your results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Evaluate each model on the test set and compare results\n",
    "# Your code here\n",
    "\n",
    "# Example comparison table structure:\n",
    "results = {\n",
    "    'Model': ['Sequential', 'Wide & Deep', 'Wide & Deep with Aux'],\n",
    "    'Test RMSE': [0, 0, 0],  # Fill with your results\n",
    "    'Training Time': [0, 0, 0]  # Fill with your results\n",
    "}\n",
    "pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bonus Challenge\n",
    "\n",
    "Modify the wide & deep architecture to include dropout layers and experiment with different learning rates. Does this improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Custom Activation Functions and Their Impact\n",
    "\n",
    "In this exercise, you'll implement custom activation functions and compare their performance with standard activations. You'll use the Wine Quality dataset, which is small but provides an interesting regression problem.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Load Wine Quality dataset\n",
    "wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "X = wine_data.drop('quality', axis=1).values\n",
    "y = wine_data['quality'].values\n",
    "\n",
    "# Split and scale the data\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Utility function for plotting\n",
    "def plot_activation_functions(activation_functions, x_range=(-5, 5)):\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for name, fn in activation_functions.items():\n",
    "        y = fn(x)\n",
    "        plt.plot(x, y, label=name)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(\"Activation Functions\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Implementing Custom Activation Functions\n",
    "\n",
    "Implement the following custom activation functions:\n",
    "\n",
    "1. Mish: f(x) = x * tanh(softplus(x))\n",
    "2. Swish: f(x) = x * sigmoid(x)\n",
    "3. A custom variant of your choice (be creative!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_mish(x):\n",
    "    # TODO: Implement the Mish activation function\n",
    "    # Hint: Use tf.math.softplus and tf.math.tanh\n",
    "    return None\n",
    "\n",
    "def custom_swish(x):\n",
    "    # TODO: Implement the Swish activation function\n",
    "    # Hint: Use tf.math.sigmoid\n",
    "    return None\n",
    "\n",
    "def custom_variant(x):\n",
    "    # TODO: Implement your own activation function variant\n",
    "    # Be creative! Consider combining existing functions or creating something new\n",
    "    return None\n",
    "\n",
    "# Test your implementations\n",
    "activation_functions = {\n",
    "    \"Mish\": custom_mish,\n",
    "    \"Swish\": custom_swish,\n",
    "    \"Custom Variant\": custom_variant\n",
    "}\n",
    "\n",
    "# Create test input\n",
    "test_input = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Test outputs:\")\n",
    "for name, fn in activation_functions.items():\n",
    "    print(f\"{name}: {fn(test_input).numpy()}\")\n",
    "\n",
    "# Plot the activation functions\n",
    "plot_activation_functions(activation_functions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Creating Models with Custom Activations\n",
    "\n",
    "Create a function that builds a model using a given activation function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(activation_fn, input_shape=[11]):\n",
    "    # TODO: Implement a model with:\n",
    "    # - 3 Dense layers (64, 32, 1 neurons)\n",
    "    # - Custom activation for hidden layers\n",
    "    # - No activation for output layer\n",
    "    # - He initialization for weights\n",
    "    return None\n",
    "\n",
    "# Create models with different activations\n",
    "activations_to_test = {\n",
    "    \"ReLU\": tf.nn.relu,\n",
    "    \"Mish\": custom_mish,\n",
    "    \"Swish\": custom_swish,\n",
    "    \"Custom\": custom_variant\n",
    "}\n",
    "\n",
    "models = {name: create_model(fn) for name, fn in activations_to_test.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Training and Comparison\n",
    "\n",
    "Train each model and compare their performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, name):\n",
    "    # TODO: Implement training and evaluation\n",
    "    # - Compile model with appropriate loss and metrics\n",
    "    # - Train for 20 epochs\n",
    "    # - Record training time and history\n",
    "    # - Evaluate on test set\n",
    "    # Return training time, history, and test score\n",
    "    return None\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining model with {name} activation:\")\n",
    "    results[name] = train_and_evaluate(model, name)\n",
    "\n",
    "# TODO: Create a comparison DataFrame with:\n",
    "# - Training time\n",
    "# - Final training loss\n",
    "# - Final validation loss\n",
    "# - Test score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Visualization and Analysis\n",
    "\n",
    "Create visualizations to compare the performance of different activation functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    # TODO: Create plots comparing:\n",
    "    # - Training loss over time\n",
    "    # - Validation loss over time\n",
    "    # - Training vs validation loss for each activation\n",
    "    pass\n",
    "\n",
    "# Create visualizations\n",
    "plot_training_curves(results)\n",
    "\n",
    "# TODO: Calculate and display additional metrics like:\n",
    "# - Training speed (examples/second)\n",
    "# - Number of parameters\n",
    "# - Memory usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5: Analysis Questions\n",
    "\n",
    "1. Which activation function performed best in terms of:\n",
    "   - Final model accuracy?\n",
    "   - Training speed?\n",
    "   - Convergence stability?\n",
    "\n",
    "2. Why do you think your custom activation function performed the way it did?\n",
    "\n",
    "3. What are the tradeoffs between the different activation functions you tested?\n",
    "\n",
    "## Bonus Challenge\n",
    "\n",
    "1. Implement a parametric version of your custom activation function where some aspects of the function can be learned during training.\n",
    "\n",
    "2. Try using your activation functions with different optimizers (Adam, SGD, RMSprop) and learning rates. Do some activation functions work better with certain optimizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Batch Normalization Placement and Effects\n",
    "\n",
    "In this exercise, you'll experiment with different batch normalization strategies and analyze their impact on model training and performance. You'll use the Fashion MNIST dataset but work with a smaller subset to keep computation manageable.\n",
    "\n",
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "# Load and preprocess Fashion MNIST data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "\n",
    "# Take a subset of the data to keep computation manageable\n",
    "X_train = X_train_full[:10000] / 255.0\n",
    "y_train = y_train_full[:10000]\n",
    "X_valid = X_train_full[10000:12000] / 255.0\n",
    "y_valid = y_train_full[10000:12000]\n",
    "X_test = X_test[:2000] / 255.0\n",
    "y_test = y_test[:2000]\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Class names for reference\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Utility function for plotting training history\n",
    "def plot_learning_curves(history, metrics=['loss', 'accuracy']):\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        ax.plot(history.history[metric], label='Train')\n",
    "        ax.plot(history.history[f'val_{metric}'], label='Validation')\n",
    "        ax.set_title(f'Model {metric.capitalize()}')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Baseline Model Without Batch Normalization\n",
    "\n",
    "First, create a baseline CNN model without any batch normalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_baseline_model():\n",
    "    # TODO: Create a CNN with:\n",
    "    # - 2 convolutional layers (32 and 64 filters)\n",
    "    # - MaxPooling after each conv layer\n",
    "    # - Dense layer with 128 neurons\n",
    "    # - Output layer with 10 neurons (softmax)\n",
    "    # Use ReLU activation for hidden layers\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid))\n",
    "\n",
    "plot_learning_curves(baseline_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Batch Normalization After Activation\n",
    "\n",
    "Implement a model with batch normalization layers placed after activation functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_bn_after_activation_model():\n",
    "    # TODO: Create a CNN with the same architecture as baseline, but add\n",
    "    # batch normalization layers after each ReLU activation\n",
    "    # Order should be: Conv -> ReLU -> BatchNorm -> MaxPool\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "bn_after_model = create_bn_after_activation_model()\n",
    "bn_after_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "bn_after_history = bn_after_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid))\n",
    "\n",
    "plot_learning_curves(bn_after_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Batch Normalization Before Activation\n",
    "\n",
    "Implement a model with batch normalization layers placed before activation functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bn_before_activation_model():\n",
    "    # TODO: Create a CNN with the same architecture as baseline, but add\n",
    "    # batch normalization layers before each ReLU activation\n",
    "    # Order should be: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "    model = None # Your code here\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "bn_before_model = create_bn_before_activation_model()\n",
    "bn_before_model.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "bn_before_history = bn_before_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid))\n",
    "\n",
    "plot_learning_curves(bn_before_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 4: Analysis\n",
    "\n",
    "Compare the performance of all three models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_models(histories, names):\n",
    "    # TODO: Create a performance comparison including:\n",
    "    # - Final training accuracy\n",
    "    # - Final validation accuracy\n",
    "    # - Training time\n",
    "    # - Number of parameters\n",
    "    # Return a pandas DataFrame with the results\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "model_histories = {\n",
    "    'Baseline': baseline_history,\n",
    "    'BN After Activation': bn_after_history,\n",
    "    'BN Before Activation': bn_before_history\n",
    "}\n",
    "\n",
    "comparison_df = compare_models(model_histories, list(model_histories.keys()))\n",
    "print(comparison_df)\n",
    "\n",
    "# TODO: Create visualizations comparing:\n",
    "# 1. Training curves (accuracy and loss) for all models\n",
    "# 2. Training time per epoch\n",
    "# 3. Parameter count comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 5: Hyperparameter Sensitivity\n",
    "\n",
    "Test how each model responds to different learning rates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_learning_rates(model_fn, learning_rates=[0.1, 0.01, 0.001]):\n",
    "    results = []\n",
    "    for lr in learning_rates:\n",
    "        # TODO: Train the model with different learning rates\n",
    "        # Record accuracy and stability metrics\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "# Test hyperparameter sensitivity for each model type\n",
    "# Implement your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Analysis Questions\n",
    "\n",
    "1. Which batch normalization placement strategy performed best? Why do you think this is the case?\n",
    "\n",
    "2. How did the different models respond to changes in learning rate? Which was most stable?\n",
    "\n",
    "3. What are the tradeoffs between the different approaches in terms of:\n",
    "   - Training time\n",
    "   - Model complexity (number of parameters)\n",
    "   - Final performance\n",
    "   - Training stability\n",
    "\n",
    "## Bonus Challenge\n",
    "\n",
    "1. Implement a version of the model that uses batch normalization with the following modifications:\n",
    "   - Custom batch normalization momentum\n",
    "   - Different initialization strategies for batch normalization parameters\n",
    "   - Experiment with different batch sizes to see how they affect batch normalization's performance\n",
    "\n",
    "2. Add dropout layers to each model variant and compare how batch normalization interacts with dropout when placed in different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Multi-Input Networks for Mixed Data Types\n",
    "\n",
    "In this exercise, you'll build a network that processes both numerical and categorical data to predict car prices using the Auto MPG dataset. You'll implement different architectures for processing each type of data and combine them effectively.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare the Auto MPG dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin', 'Car Name']\n",
    "\n",
    "# Load the raw data\n",
    "df = pd.read_csv(url, names=column_names,\n",
    "                 na_values='?', comment='\\t',\n",
    "                 sep=' ', skipinitialspace=True)\n",
    "\n",
    "# Clean the data\n",
    "df = df.dropna()\n",
    "\n",
    "# Separate features\n",
    "numeric_features = ['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']\n",
    "categorical_features = ['Model Year', 'Origin']\n",
    "target = 'MPG'\n",
    "\n",
    "# Create inputs\n",
    "numeric_data = df[numeric_features]\n",
    "categorical_data = df[categorical_features]\n",
    "target_data = df[target]\n",
    "\n",
    "# Split the data\n",
    "X_num_train_full, X_num_test, X_cat_train_full, X_cat_test, y_train_full, y_test = train_test_split(\n",
    "    numeric_data, categorical_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_num_train, X_num_valid, X_cat_train, X_cat_valid, y_train, y_valid = train_test_split(\n",
    "    X_num_train_full, X_cat_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numeric data\n",
    "scaler = StandardScaler()\n",
    "X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "X_num_valid_scaled = scaler.transform(X_num_valid)\n",
    "X_num_test_scaled = scaler.transform(X_num_test)\n",
    "\n",
    "# Encode categorical data\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_cat_train_encoded = encoder.fit_transform(X_cat_train)\n",
    "X_cat_valid_encoded = encoder.transform(X_cat_valid)\n",
    "X_cat_test_encoded = encoder.transform(X_cat_test)\n",
    "\n",
    "# Utility function for plotting training history\n",
    "def plot_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(history.history['mape'], label='Training MAPE')\n",
    "    ax2.plot(history.history['val_mape'], label='Validation MAPE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAPE')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Basic Multi-Input Model\n",
    "\n",
    "Create a basic multi-input model that processes numeric and categorical data separately:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_basic_multi_input_model(num_features, cat_features):\n",
    "    # TODO: Create a model with:\n",
    "    # - One input branch for numeric features\n",
    "    # - One input branch for categorical features\n",
    "    # - Dense layers for each branch\n",
    "    # - Concatenated outputs\n",
    "    # - Final prediction layer\n",
    "    # Return both the model and its inputs\n",
    "    return None, None\n",
    "\n",
    "# Create and compile the model\n",
    "num_features = X_num_train_scaled.shape[1]\n",
    "cat_features = X_cat_train_encoded.shape[1]\n",
    "\n",
    "model, inputs = create_basic_multi_input_model(num_features, cat_features)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mape'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_num_train_scaled, X_cat_train_encoded],\n",
    "    y_train,\n",
    "    validation_data=([X_num_valid_scaled, X_cat_valid_encoded], y_valid),\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Enhanced Multi-Input Model with Different Branch Depths\n",
    "\n",
    "Create a more sophisticated model with different architectures for numeric and categorical data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_enhanced_multi_input_model(num_features, cat_features):\n",
    "    # TODO: Create a model with:\n",
    "    # - Deep branch for numeric features (3+ layers)\n",
    "    # - Shallow branch for categorical features (1-2 layers)\n",
    "    # - Batch normalization in numeric branch\n",
    "    # - Dropout in both branches\n",
    "    # - Skip connection in numeric branch\n",
    "    # Return both the model and its inputs\n",
    "    return None, None\n",
    "\n",
    "# Create and compile the enhanced model\n",
    "model_enhanced, inputs_enhanced = create_enhanced_multi_input_model(num_features, cat_features)\n",
    "model_enhanced.compile(optimizer='adam', loss='mse', metrics=['mape'])\n",
    "\n",
    "# Train the enhanced model\n",
    "history_enhanced = model_enhanced.fit(\n",
    "    [X_num_train_scaled, X_cat_train_encoded],\n",
    "    y_train,\n",
    "    validation_data=([X_num_valid_scaled, X_cat_valid_encoded], y_valid),\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "plot_learning_curves(history_enhanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Adding Auxiliary Outputs\n",
    "\n",
    "Modify your enhanced model to include auxiliary outputs for regularization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_multi_output_model(num_features, cat_features):\n",
    "    # TODO: Create a model with:\n",
    "    # - Similar structure to enhanced model\n",
    "    # - Auxiliary output from numeric branch\n",
    "    # - Auxiliary output from categorical branch\n",
    "    # - Main output from combined features\n",
    "    # Return model and inputs\n",
    "    return None, None\n",
    "\n",
    "# Create and compile the multi-output model\n",
    "model_multi_output, inputs_multi_output = create_multi_output_model(num_features, cat_features)\n",
    "model_multi_output.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mse', 'mse', 'mse'],\n",
    "    loss_weights=[0.7, 0.15, 0.15],\n",
    "    metrics=['mape']\n",
    ")\n",
    "\n",
    "# Train the multi-output model\n",
    "history_multi_output = model_multi_output.fit(\n",
    "    [X_num_train_scaled, X_cat_train_encoded],\n",
    "    [y_train, y_train, y_train],\n",
    "    validation_data=(\n",
    "        [X_num_valid_scaled, X_cat_valid_encoded],\n",
    "        [y_valid, y_valid, y_valid]\n",
    "    ),\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "plot_learning_curves(history_multi_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Model Comparison and Analysis\n",
    "\n",
    "Compare the performance of all three models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_models(models, histories, X_test_data, y_test_data):\n",
    "    # TODO: Create a comparison including:\n",
    "    # - Test set performance\n",
    "    # - Training time\n",
    "    # - Number of parameters\n",
    "    # - Best validation performance\n",
    "    # Return a pandas DataFrame with results\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "models = {\n",
    "    'Basic': model,\n",
    "    'Enhanced': model_enhanced,\n",
    "    'Multi-Output': model_multi_output\n",
    "}\n",
    "\n",
    "histories = {\n",
    "    'Basic': history,\n",
    "    'Enhanced': history_enhanced,\n",
    "    'Multi-Output': history_multi_output\n",
    "}\n",
    "\n",
    "comparison_df = compare_models(\n",
    "    models, histories,\n",
    "    [X_num_test_scaled, X_cat_test_encoded],\n",
    "    y_test\n",
    ")\n",
    "print(comparison_df)\n",
    "\n",
    "# TODO: Create visualizations comparing:\n",
    "# 1. Learning curves for all models\n",
    "# 2. Prediction accuracy on test set\n",
    "# 3. Training time comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5: Feature Importance Analysis\n",
    "\n",
    "Analyze which features contribute most to the predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    # TODO: Implement feature importance analysis\n",
    "    # Hint: Use partial dependence plots or permutation importance\n",
    "    pass\n",
    "\n",
    "# Analyze feature importance for each model\n",
    "numeric_names = numeric_features\n",
    "categorical_names = [f\"{feat}_{val}\" for feat, vals in \n",
    "                    zip(categorical_features, encoder.categories_) \n",
    "                    for val in vals]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis Questions\n",
    "\n",
    "1. How did the different architectures affect model performance?\n",
    "2. Which features were most important for predicting MPG?\n",
    "3. Did the auxiliary outputs improve model performance? Why or why not?\n",
    "4. What are the tradeoffs between the three architectures in terms of:\n",
    "   - Training time\n",
    "   - Model complexity\n",
    "   - Prediction accuracy\n",
    "   - Feature interpretability\n",
    "\n",
    "## Bonus Challenge\n",
    "\n",
    "1. Implement a version of the model that:\n",
    "   - Uses different optimizers for different branches\n",
    "   - Implements custom regularization\n",
    "   - Uses a custom loss function that penalizes large errors more heavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Analyzing Gradient Flow in Deep Networks\n",
    "\n",
    "In this exercise, you'll build deep networks with different activation functions and analyze how gradients flow through them. You'll create tools to visualize gradient behavior and compare how different activation functions affect training dynamics.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Create a simple synthetic dataset\n",
    "np.random.seed(42)\n",
    "N_SAMPLES = 1000\n",
    "N_FEATURES = 10\n",
    "\n",
    "# Generate synthetic data with clear patterns\n",
    "X = np.random.randn(N_SAMPLES, N_FEATURES)\n",
    "# Create a non-linear relationship\n",
    "y = np.sin(X[:, 0]) + np.square(X[:, 1]) + np.exp(-X[:, 2]) + np.random.randn(N_SAMPLES) * 0.1\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train = X[:800]\n",
    "y_train = y[:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]\n",
    "\n",
    "# Custom callback to track gradients\n",
    "class GradientCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(GradientCallback, self).__init__()\n",
    "        self.gradients = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        weights = self.model.trainable_weights\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(X_train[:32])\n",
    "            loss = tf.keras.losses.mean_squared_error(y_train[:32], predictions)\n",
    "        grads = tape.gradient(loss, weights)\n",
    "        self.gradients.append([tf.reduce_mean(tf.abs(g)) for g in grads if g is not None])\n",
    "\n",
    "# Utility function for plotting\n",
    "def plot_gradient_flow(gradients, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gradients)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Mean Gradient Magnitude')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Building Deep Networks with Different Activations\n",
    "\n",
    "Create a function to build deep networks with different activation functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_deep_network(activation='relu', n_layers=50):\n",
    "    # TODO: Create a deep network with:\n",
    "    # - n_layers dense layers (each with 32 neurons)\n",
    "    # - Specified activation function\n",
    "    # - Proper initialization based on activation\n",
    "    # - Input shape of (N_FEATURES,)\n",
    "    # - Single output neuron\n",
    "    # Use appropriate weight initialization for each activation\n",
    "    return None\n",
    "\n",
    "# Test different activation functions\n",
    "activation_functions = ['relu', 'tanh', 'selu']\n",
    "models = {}\n",
    "for activation in activation_functions:\n",
    "    # TODO: Create and compile model\n",
    "    # Store in models dictionary\n",
    "    pass\n",
    "\n",
    "# Training function\n",
    "def train_model_with_gradient_tracking(model, X_train, y_train, epochs=10):\n",
    "    # TODO: Implement training with gradient tracking\n",
    "    # Return history and gradient information\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Gradient Flow Analysis\n",
    "\n",
    "Implement functions to analyze gradient flow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_gradient_statistics(gradients):\n",
    "    # TODO: Calculate and return gradient statistics including:\n",
    "    # - Mean gradient magnitude per layer\n",
    "    # - Gradient variance per layer\n",
    "    # - Layer-wise gradient ratio (consecutive layers)\n",
    "    # Return as pandas DataFrame\n",
    "    return None\n",
    "\n",
    "def plot_gradient_heatmap(gradients, activation_name):\n",
    "    # TODO: Create a heatmap showing gradient magnitudes across:\n",
    "    # - Layers (y-axis)\n",
    "    # - Training steps (x-axis)\n",
    "    pass\n",
    "\n",
    "# Analyze each model\n",
    "for activation, model in models.items():\n",
    "    # TODO: Train model and collect gradient information\n",
    "    # Create visualizations\n",
    "    # Print statistics\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Layer-wise Gradient Evolution\n",
    "\n",
    "Create visualizations showing how gradients evolve through layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_layer_gradients(model_gradients, layer_indices=[0, 25, 49]):\n",
    "    # TODO: Create plots showing:\n",
    "    # - Gradient distribution per selected layer\n",
    "    # - Gradient magnitude changes over time\n",
    "    # - Compare early, middle, and late layers\n",
    "    pass\n",
    "\n",
    "def analyze_gradient_vanishing(gradients):\n",
    "    # TODO: Implement metrics to quantify gradient vanishing:\n",
    "    # - Gradient magnitude ratio (last layer / first layer)\n",
    "    # - Layer depth where gradients become very small\n",
    "    # Return metrics as dictionary\n",
    "    return None\n",
    "\n",
    "# Analyze gradient evolution for each model\n",
    "for activation, model in models.items():\n",
    "    # TODO: Generate and plot gradient evolution analysis\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Comparative Analysis\n",
    "\n",
    "Compare the performance of networks with different activations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_models_performance(models, X_train, y_train, X_test, y_test):\n",
    "    # TODO: Create a comparison including:\n",
    "    # - Training time\n",
    "    # - Final training loss\n",
    "    # - Test loss\n",
    "    # - Gradient stability metrics\n",
    "    # Return as pandas DataFrame\n",
    "    return None\n",
    "\n",
    "# Create comparative visualizations\n",
    "def plot_comparative_results(training_results):\n",
    "    # TODO: Create subplots comparing:\n",
    "    # - Training curves\n",
    "    # - Gradient flow characteristics\n",
    "    # - Final performance metrics\n",
    "    pass\n",
    "\n",
    "# Run comparison\n",
    "results = compare_models_performance(models, X_train, y_train, X_test, y_test)\n",
    "plot_comparative_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5: Improving Gradient Flow\n",
    "\n",
    "Implement techniques to improve gradient flow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_residual_network(activation='relu', n_layers=50):\n",
    "    # TODO: Implement a deep network with residual connections\n",
    "    # Compare with basic deep network\n",
    "    return None\n",
    "\n",
    "def create_highway_network(activation='relu', n_layers=50):\n",
    "    # TODO: Implement a highway network\n",
    "    # Compare with basic deep network\n",
    "    return None\n",
    "\n",
    "# Compare improved architectures\n",
    "improved_models = {\n",
    "    'residual': create_residual_network(),\n",
    "    'highway': create_highway_network()\n",
    "}\n",
    "\n",
    "# Compare with original models\n",
    "final_comparison = compare_models_performance(\n",
    "    {**models, **improved_models},\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis Questions\n",
    "\n",
    "1. Which activation function showed the most stable gradient flow? Why?\n",
    "2. At what depth did gradients start vanishing for each activation function?\n",
    "3. How did residual/highway connections affect gradient flow?\n",
    "4. What are the practical implications of your findings for designing deep networks?\n",
    "\n",
    "## Bonus Challenge\n",
    "\n",
    "1. Implement a custom activation function designed to maintain stable gradients\n",
    "2. Create an adaptive learning rate scheme based on gradient statistics\n",
    "3. Visualize gradient flow in a deep convolutional network"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
