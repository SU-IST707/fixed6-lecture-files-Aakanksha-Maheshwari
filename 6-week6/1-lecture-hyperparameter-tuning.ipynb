{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning**\n",
    "\n",
    "In machine learning, model parameters are learned from the data automatically during training. For instance, the weights in a linear regression or a neural network are parameters. Hyperparameters, on the other hand, are configurations external to the model, which can't be estimated from data. They are often set before the learning process begins.\n",
    "\n",
    "The performance of a machine learning model can be sensitive to the hyperparameters provided. Therefore, finding the optimal hyperparameters is crucial. This process of searching for the ideal model hyperparameters is called hyperparameter tuning or optimization.\n",
    "\n",
    "#### **Why Is Hyperparameter Tuning Important?**\n",
    "\n",
    "- **Performance**: Properly tuned hyperparameters can significantly improve the performance of a modelâ€”conversely, poorly set hyperparameters can render even a well-structured model ineffective.\n",
    "  \n",
    "- **Overfitting vs. Underfitting**: Hyperparameters can influence model complexity. For example, a high polynomial degree might fit training data perfectly but fail on test data. Hyperparameter tuning aids in finding a balanced model that generalizes well.\n",
    "\n",
    "- **Computational Efficiency**: Some hyperparameters can influence how fast a model trains. For example, the learning rate in many algorithms determines how fast they converge to a solution.\n",
    "\n",
    "#### **Common Methods for Hyperparameter Tuning**:\n",
    "\n",
    "1. **Manual Search**: Initially, practitioners often set hyperparameters based on intuition or experience. Though not systematic, it provides a good starting point.\n",
    "\n",
    "2. **Grid Search**: A brute-force approach, where you specify a subset of the hyperparameter space. It then evaluates the model performance for each point in the grid.\n",
    "\n",
    "3. **Random Search**: Instead of a comprehensive search like Grid Search, Random Search randomly selects points in the hyperparameter space and checks their performance. It can outperform Grid Search, especially when only a few hyperparameters matter.\n",
    "\n",
    "4. **Bayesian Optimization**: This probabilistic model-based approach balances exploration and exploitation. It builds a probability model of the objective function and uses it to select hyperparameters that might perform well.\n",
    "\n",
    "#### **State-of-the-Art and Sensitivity-based Approaches**:\n",
    "\n",
    "Bayesian Optimization is among the more advanced methods and is considered state-of-the-art for hyperparameter tuning. It's especially useful when evaluating the objective function (like the validation error) is costly.\n",
    "\n",
    "Another tool to use with hyperparameter tuning is *sensitivity analysis*.  Sensitivity analysis describes the process of determining how important (or sensitive) and output function is to a given parameter. The principles of sensitivity analysis are somewhat applied in methods like Bayesian Optimization, where the aim is to intelligently sample the hyperparameters that are likely to yield the best performance.\n",
    "\n",
    "In Scikit-Learn, there isn't a direct implementation of sensitivity analysis for hyperparameter tuning. However, there are more specialized libraries or tools that focus on Bayesian Optimization or other advanced optimization techniques. Libraries such as `Scikit-Optimize` and `Hyperopt` are examples of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "1. **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data,columns=wine.feature_names)\n",
    "y = wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.5, 'learning_rate': 0.5, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'objective': 'multi:softmax', 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.5],\n",
    "    'max_depth': [3, 7],\n",
    "    'min_child_weight': [3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators' : [100, 200],\n",
    "    'objective': ['multi:softmax']\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \",grid_search.best_params_)\n",
    "print(\"Best score\",grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to use this to create a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the best model, you can use:\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Alternatively, you can initialize a new model like:\n",
    "new_model = xgb.XGBClassifier(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Random Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'objective': 'multi:softmax', 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.56, 'colsample_bytree': 0.9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': np.linspace(0.01, 1, 10),\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5],\n",
    "    'subsample': np.linspace(0.5, 1, 6),\n",
    "    'colsample_bytree': np.linspace(0.5, 1, 6),\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'objective': ['multi:softmax']\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring='accuracy', cv=3, verbose=1, n_iter=50)\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \",rand_search.best_params_)\n",
    "print(\"Best score\",rand_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929078014184397"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Bayesian Optimization**\n",
    "\n",
    "Although Bayesian Optimization is not part of the sklearn library, there is a popular standalone Python library called `bayesian-optimization` that is often used in conjunction with `scikit-learn` for this purpose.\n",
    "\n",
    "   Before we can use `bayesian-optimization`, you must install it:\n",
    "\n",
    "   ```bash\n",
    "   pip install bayesian-optimization\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/jeintron/Dropbox/@TEACHING/2023/FALL23/DEV/ist407-707.venv/lib/python3.10/site-packages (from bayesian-optimization) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/jeintron/Dropbox/@TEACHING/2023/FALL23/DEV/ist407-707.venv/lib/python3.10/site-packages (from bayesian-optimization) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/jeintron/Dropbox/@TEACHING/2023/FALL23/DEV/ist407-707.venv/lib/python3.10/site-packages (from bayesian-optimization) (1.3.0)\n",
      "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/jeintron/Dropbox/@TEACHING/2023/FALL23/DEV/ist407-707.venv/lib/python3.10/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jeintron/Dropbox/@TEACHING/2023/FALL23/DEV/ist407-707.venv/lib/python3.10/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n",
      "Installing collected packages: colorama, bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings, as there are quite a few here\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m0.3613   \u001b[0m | \u001b[0m0.5095   \u001b[0m | \u001b[0m0.217    \u001b[0m | \u001b[0m7.949    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.6242   \u001b[0m | \u001b[95m0.5326   \u001b[0m | \u001b[95m0.29     \u001b[0m | \u001b[95m4.637    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m0.7337   \u001b[0m | \u001b[0m0.4053   \u001b[0m | \u001b[0m0.08662  \u001b[0m | \u001b[0m3.435    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9444   \u001b[0m | \u001b[0m0.8527   \u001b[0m | \u001b[0m0.7104   \u001b[0m | \u001b[0m0.0792   \u001b[0m | \u001b[0m8.202    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.4841   \u001b[0m | \u001b[0m0.2463   \u001b[0m | \u001b[0m0.2483   \u001b[0m | \u001b[0m8.349    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.7366   \u001b[0m | \u001b[0m0.3126   \u001b[0m | \u001b[0m0.03078  \u001b[0m | \u001b[0m5.099    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.63     \u001b[0m | \u001b[0m0.2619   \u001b[0m | \u001b[0m0.08581  \u001b[0m | \u001b[0m7.102    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m0.8113   \u001b[0m | \u001b[0m0.2522   \u001b[0m | \u001b[0m0.2628   \u001b[0m | \u001b[0m5.819    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.3653   \u001b[0m | \u001b[0m0.6476   \u001b[0m | \u001b[0m0.2067   \u001b[0m | \u001b[0m9.778    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.7467   \u001b[0m | \u001b[0m0.9161   \u001b[0m | \u001b[0m0.2495   \u001b[0m | \u001b[0m6.524    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m0.7477   \u001b[0m | \u001b[0m0.2521   \u001b[0m | \u001b[0m0.209    \u001b[0m | \u001b[0m5.07     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.5828   \u001b[0m | \u001b[0m0.2009   \u001b[0m | \u001b[0m0.1195   \u001b[0m | \u001b[0m9.397    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.5071   \u001b[0m | \u001b[0m0.2944   \u001b[0m | \u001b[0m0.2543   \u001b[0m | \u001b[0m8.664    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.6493   \u001b[0m | \u001b[0m0.362    \u001b[0m | \u001b[0m0.2023   \u001b[0m | \u001b[0m9.498    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9444   \u001b[0m | \u001b[0m0.8758   \u001b[0m | \u001b[0m0.2883   \u001b[0m | \u001b[0m0.1557   \u001b[0m | \u001b[0m5.04     \u001b[0m |\n",
      "=========================================================================\n",
      "Optimized parameters: {'colsample_bytree': 0.6242000666592495, 'gamma': 0.5325570297687131, 'learning_rate': 0.2900023537199075, 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "wine = load_wine(as_frame=True)\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Objective function for Bayesian Optimization\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, learning_rate):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'learning_rate': learning_rate,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'silent': 1\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Bayesian Optimization\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 0.9),\n",
    "    'learning_rate': (0.01, 0.3)\n",
    "})\n",
    "\n",
    "# Maximize the objective function\n",
    "xgb_bo.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# Results\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "print(\"Optimized parameters:\", params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for working with pipelines in SKLearn\n",
    "\n",
    "For using parameter tuning in a pipeline with `sklearn`, here are a few considerations:\n",
    "\n",
    "1. **Name your pipeline steps**: When using `Pipeline` in `sklearn`, each step is given a name (either by you or by default). When specifying hyperparameters for grid search or random search, you'll need to use these names.\n",
    "\n",
    "    For example, if your pipeline is:\n",
    "    ```python\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', XGBClassifier())\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "    Then, a parameter grid for grid search might look like:\n",
    "    ```python\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "2. **Pipeline in `GridSearchCV` and `RandomizedSearchCV`**: You can indeed directly pass a pipeline into these search methods in `sklearn`. This is very handy as it allows you to search not just over model parameters, but also over preprocessing steps and their parameters.\n",
    "\n",
    "3. **Complexity & Runtime**: Keep in mind that hyperparameter tuning can significantly increase the runtime, especially with a large parameter grid or a large dataset. This is even more pronounced with Bayesian optimization because the optimization procedure itself also adds overhead. This is particularly true when you combine this with cross-validation. Always start with a smaller subset or a smaller parameter space to gauge the time it will take.\n",
    "\n",
    "4. **Consistent Seeds**: If you want reproducibility, make sure to set random seeds consistently, especially if the methods you're using (like `RandomizedSearchCV` or the model itself) have stochastic behavior.\n",
    "\n",
    "5. **Nested CV**: If you're using cross-validation within the Bayesian Optimization and also wish to evaluate the model's performance using cross-validation, you'll end up with a nested cross-validation setup. Nested CV is a robust way to estimate the performance of the entire process (including hyperparameter tuning), but it's computationally expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
