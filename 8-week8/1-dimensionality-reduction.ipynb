{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a technique used in machine learning and statistics to reduce the number of input variables in a dataset. More input features often make a model more complex, increasing the risk of overfitting and making the model harder to interpret. Dimensionality reduction techniques aim to simplify models without losing much information.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Dimensionality Reduction Important?\n",
    "\n",
    "1. **Simplification**: Makes the dataset easier to explore and visualize.\n",
    "2. **Speed**: Reduces the computational complexity, making algorithms run faster.\n",
    "3. **Data Compression**: Allows for more efficient storage of data.\n",
    "4. **Noise Reduction**: Helps to eliminate irrelevant features or reduce noise.\n",
    "5. **Improved Performance**: Can lead to better model performance when irrelevant features are removed.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Customer Segmentation in Retail\n",
    "\n",
    "Imagine you're a data scientist at a retail company. The company collects data on customer transactions, online activity, customer service interactions, and more. In total, you have hundreds of features for each customer.\n",
    "\n",
    "**The Problem**: You want to segment your customer base to target marketing more effectively, but the high dimensionality of your data makes it difficult to gain any meaningful insights.\n",
    "\n",
    "**The Solution**: By applying dimensionality reduction techniques like PCA, you can reduce your hundreds of features to just a few principal components. These components can effectively summarize the essential information in the dataset, making it easier to perform customer segmentation.\n",
    "\n",
    "**Outcome**: The marketing team can now more effectively tailor strategies for different segments, thereby increasing customer engagement and revenue.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. The curse of dimensionality\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Adaptive Methods: t-SNE and UMAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    " In high dimensions, the behavior of data can be counterintuitive, and algorithms that work well in low dimensions can become ineffective or computationally expensive.  This is because the available \"space\" that training samples are embedded grows exponentially with respect to the data.  This has several ramifications:\n",
    "\n",
    " - Distances between items become larger\n",
    " - Difference between distances become smaller\n",
    " - The amount of data required to cover the space grows exponentially\n",
    "\n",
    " See [this explanation](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/) to help guide your intuitions.\n",
    "\n",
    "### The Hughes Phenomenon ###\n",
    "\n",
    "In the late 70’s, G. Hughes observed a strange phonomenon while observing large datasets. It is best summarized by the picture below. \n",
    "\n",
    "![The Hughes Phenomenon](./assets/hughes_phenomenon.png)\n",
    "\n",
    "Intuitively, it would be easy to understand that the more inputs (features) you provide the model, the more the predictive power of the model. However, after a certain point the accuracy of the prediction drops off. This is the essence of Hughes Phenomonon. It is based on the fact that measuring data (features or variables) typically has some level of error to it. When you compound this error over a large number of variables, the error explodes so much that the accuracy is affected.\n",
    "\n",
    "### It Depends!\n",
    "\n",
    "The curse of dimensionality depends heavily on the distribution of data in a space, the measurements used to calculate distance, and the algorithm applied.  For instance, KNN is highly susceptible to the curse of dimensionality, and Euclidean distances tend to perform worse than either Manhattan or cosine.  So, it's always worth exploring your data a little before you decide whether or not to worry about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experiments with dimensionality\n",
    "\n",
    "The following code illustrates how increasing dimensionality increases distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_points(num_points, num_dimensions):\n",
    "    return[np.random.randint(-100, 100, num_dimensions) for _ in range(num_points)]\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def compute_pairwise_distances(points_x,points_y):\n",
    "    distances = []\n",
    "    for i,j in zip(points_x,points_y):\n",
    "        distances.append(euclidean_distance(i,j))\n",
    "    return np.array(distances)\n",
    "\n",
    "# Set the number of points and dimensions\n",
    "num_points = 10000\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Loop through a few different dimensions\n",
    "for ax, num_dimensions in zip(axes, [2, 5, 50]):\n",
    "    # Generate points in unit hypercube\n",
    "    points_x = generate_points(num_points, num_dimensions)\n",
    "    points_y = generate_points(num_points, num_dimensions)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    distances = compute_pairwise_distances(points_x,points_y)\n",
    "    \n",
    "    # Plot histogram of distances\n",
    "    ax.hist(distances, bins=30, edgecolor='black')\n",
    "    ax.set_title(f\"{num_dimensions}-D Space\")\n",
    "    ax.set_xlabel(\"Distance\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Can you create a loop and evaluate the mean distance for dimensions from 0 - 500? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, observe that the relative differences between max an min in fact approach zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = np.random.randint(-100,100,N)\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mxd = max(diffs)\n",
    "    mnd = min(diffs)\n",
    "    delta = (mxd-mnd)/mnd\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Principle Components Analysis (PCA)\n",
    "\n",
    "One way to reduce dimensionality is a principle components analysis (PCA).  PCA works by performing an eigenvector/eigenvalue decomposition of the data's covariance matrix. The goal is to transform the original features into a new set of uncorrelated features, known as principal components, which capture as much of the data's variance as possible. The first few principal components typically capture the majority of the variance, allowing for a lower-dimensional representation of the data.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Center the data around the origin by subtracting the mean of each feature from the data points.\n",
    "\n",
    "2. Calculate the covariance matrix, which captures how each feature varies with every other feature.\n",
    "\n",
    "3. Compute the eigenvalues and eigenvectors of this covariance matrix. An eigenvector of a square matrix $A$ is a non-zero vector $\\mathbf{v}$ that, when the matrix multiplies it, only scales the vector and does not change its direction. Mathematically, this can be written as $A\\mathbf{v} = \\lambda \\mathbf{v}$\n",
    "\n",
    "   Here, $A$ is the square matrix, $\\mathbf{v}$ is the eigenvector, and $\\lambda$ is the eigenvalue corresponding to this eigenvector. The eigenvalue controls how much the eigenvector is scaled. \n",
    "   \n",
    "   **Intuition**: Imagine you have a rubber sheet that you can stretch, compress, or rotate. If you put an arrow (vector) on this sheet and then transform the sheet, most arrows would change both direction and length. However, some special arrows (eigenvectors) would only get \"stretched\" or \"compressed\" — they wouldn't change direction. The amount by which they get stretched or compressed is the eigenvalue. \n",
    "\n",
    "4. Eigenvalues are sorted in descending order, and the eigenvectors are rearranged correspondingly. The first eigenvalue will be the largest and indicates the maximum variance in the data that the first principal component (the corresponding eigenvector) captures.\n",
    "\n",
    "5. To reduce dimensions, you can now select the first $k$ eigenvectors, where $k$ is the number of dimensions you want in your reduced dataset. You then project your original, centered data into this new \\(k\\)-dimensional subspace.\n",
    "\n",
    "6. You can approximate the original data from the reduced data by projecting it back onto the original high-dimensional space by multiplying (dot-product) the transformed data with the transpose of the reduced eigenvector matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "First, we'll set up some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Create synthetic 2D data\n",
    "np.random.seed(0)\n",
    "X = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the PCA, and the reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Apply PCA and plot the rotated data\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply PCA and plot the rotated data\n",
    "X_reduced = PCA(n_components=1).fit_transform(X)\n",
    "X_reconstructed = PCA(n_components=1).fit(X).inverse_transform(X_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "plt.title(\"After PCA Rotation\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.6)\n",
    "plt.title(\"Reduced to One Dimension\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the right number of components\n",
    "\n",
    "To select the right number of components, we can use the eigenvalues to calculate the amount of variance explained.  The variance explained by each principal component is directly related to the eigenvalues of the covariance matrix of the original dataset. Specifically, the proportion of variance explained by the $i^{th}$ principal component is given by:\n",
    "\n",
    "$$\n",
    "\\text{Variance Explained}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{n} \\lambda_j}\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ is the eigenvalue corresponding to the $i^{th}$ principal component and $n$ is the number of components.\n",
    "\n",
    "The eigenvalues are stored in decreasing order, and they measure the amount of variance along each principal component. A larger eigenvalue indicates that more variance is explained by that principal component.  We can visualize variance explained using a scree plot, and use the \"elbow\" method to select the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Create synthetic 10D data\n",
    "np.random.seed(0)\n",
    "X_high_dim = np.dot(np.random.rand(10, 10), np.random.randn(10, 200)).T\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca_high_dim = PCA()  # although you can specify n_components, leaving out this parameter returns all\n",
    "X_pca_high_dim = pca_high_dim.fit_transform(X_high_dim)\n",
    "\n",
    "# Step 3: Calculate Variance Explained\n",
    "variance_explained = pca_high_dim.explained_variance_ratio_\n",
    "print(\"Variance Explained per Principal Component:\", variance_explained)\n",
    "\n",
    "# Step 4: Scree Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(variance_explained) + 1), variance_explained)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.title('Scree Plot')\n",
    "plt.xticks(range(1, len(variance_explained) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you know how much variance you want to explain, you can easily calculate it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative variance explained\n",
    "cumulative_variance_explained = np.cumsum(pca_high_dim.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components for desired explained variance\n",
    "threshold = 0.95  # 95% variance\n",
    "n_components = np.argmax(cumulative_variance_explained >= threshold) + 1\n",
    "print(f\"Number of components needed for {threshold*100}% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn also let's you specify the variance you want in advance and will automatically lift out the number of components you need, as long as your svd_solver is \"full\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_high_dim = PCA(threshold)  # although you can specify n_components, leaving out this parameter returns all\n",
    "X_pca_high_dim_auto = pca_high_dim.fit_transform(X_high_dim)\n",
    "\n",
    "print(f\"Number of components retrieved for {threshold*100}% = {X_pca_high_dim_auto.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The MNIST data contains a large sample of handwritten letters.  Each letter is a 28x28 (768) grid of pixels, so the data is quite high dimensional.  Try using PCA to visualize the data.  I'll get you started with a little code below/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load MNIST data\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"].values, mnist[\"target\"].astype(int).values\n",
    "\n",
    "# Sample the dataset to speed up computation (Optional)\n",
    "X, y = X[:7000], y[:7000]\n",
    "\n",
    "# Split the dataset\n",
    "X_mnist_train, X_mnist_test, y_mnist_train, y_mnist_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Visualize using PCA (2 components)\n",
    "# Reduce the data with PCA\n",
    "\n",
    "# You need to populate the following variable\n",
    "X__mnist_reduced = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plotting\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_mnist_reduced[:, 0], X_mnist_reduced[:, 1], c=y_mnist_train, cmap=\"jet\",s=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"MNIST - 2D PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the scree plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Scree Plot\n",
    "pca_scree = None # What goes here???\n",
    "# How do you get variance explained?\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of components to achieve 95% variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Number of components for 95% variance\n",
    "threshold = 0.95\n",
    "n_components = None # What goes here???\n",
    "print(f\"Number of components for {threshold*100}% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to look at how a transform and reconstruction affects a single MNIST digit.  I've written a function to display a single digit.  Use PCA and the reconstruction method to have a look at this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digit(digit):\n",
    "    \"\"\"Plot a single MNIST digit.\n",
    "\n",
    "    Parameters:\n",
    "    digit (numpy array): A flattened 1D numpy array of length 784.\n",
    "\n",
    "    \"\"\"\n",
    "    # Reshape the flattened digit to 28x28 image\n",
    "    digit_image = digit.reshape(28, 28)\n",
    "    \n",
    "    plt.imshow(digit_image, cmap='binary')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_mnist_digit(X_mnist_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA with n_components to transform the above digit.  How does it look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `n_components` to train a KNNClassifier, and compare accuracy to the non-reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: k-NN Classification\n",
    "# Without PCA\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_mnist_train, y_mnist_train)\n",
    "y_pred = knn.predict(X_mnist_test)\n",
    "print(f\"Original Data Accuracy: {accuracy_score(y_mnist_test, y_pred)}\")\n",
    "\n",
    "# With PCA\n",
    "# You have some work to do right here\n",
    "\n",
    "\n",
    "y_pred_pca = None # How do you do this?\n",
    "print(f\"PCA-transformed Data Accuracy: {accuracy_score(y_mnist_test, y_pred_pca)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Limitations of PCA\n",
    "\n",
    "One of the key limitations of PCA is that it is a linear technique. This means that it works by finding new axes, called principal components, that are linear combinations of the original features. These principal components aim to capture as much variance in the data as possible. However, the linear nature of PCA imposes a limitation: it struggles to capture patterns in data that are fundamentally non-linear.\n",
    "\n",
    "Imagine a dataset where points are distributed in a non-linear fashion, like a spiral or concentric circles. A linear method like PCA would not be able to capture the essence of such patterns. This is because the concept of \"distance\" or \"variance\" that PCA relies on does not adequately describe the intrinsic geometry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic spiral data\n",
    "n_points = 1000\n",
    "n_turns = 3\n",
    "t = np.linspace(0, n_turns * np.pi, n_points)\n",
    "x = t * np.cos(t)\n",
    "y = t * np.sin(t)\n",
    "\n",
    "X_spiral = np.column_stack([x, y])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_spiral)\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "\n",
    "# Original Data vs PCA reconstruction\n",
    "X_reduced = PCA(n_components=1).fit_transform(X_spiral)\n",
    "X_reconstructed = PCA(n_components=1).fit(X_spiral).inverse_transform(X_reduced)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_spiral[:, 0], X_spiral[:, 1], alpha=0.6, label='Original Data')\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.6, label='PCA Reconstructed')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.title(\"Original vs PCA Reconstructed Data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Other Approaches\n",
    "\n",
    "There are many many approach to dimesionality reduction.  Several extensions to PCA address it's limitations and extend it's capabilities.  One of these is **Kernel PCA**.\n",
    "\n",
    "### Kernel PCA\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) is an extension of the standard Principal Component Analysis (PCA), designed to handle non-linear data effectively. While standard PCA works by finding linear principal components to project data onto, Kernel PCA works in an implicitly defined higher-dimensional feature space where these projections can be non-linear.\n",
    "\n",
    "### How Kernel PCA Works\n",
    "\n",
    "1. **Feature Space Mapping**: First, each data point in the original space is mapped into a higher-dimensional feature space using a kernel function $K(x_i, x_j)$. These are the same as with **Support Vector Machines**.\n",
    "\n",
    "2. **Principal Component Analysis in Feature Space**: In this new feature space, the regular PCA algorithm is applied. Since working in a higher-dimensional space can be computationally intensive, Kernel PCA cleverly utilizes the \"kernel trick\" to compute principal components in the feature space without explicitly working in it.\n",
    "\n",
    "3. **Projecting Data**: Finally, the original data is projected onto these new principal components in the feature space. The resulting data can be linearly separable even if the original data was not.\n",
    "\n",
    "### Kernel Functions\n",
    "\n",
    "Kernel PCA relies on the use of a kernel function to compute the similarity between pairs of data points in the feature space. Popular choices for kernel functions include:\n",
    "\n",
    "- **Polynomial Kernel**: $K(x_i, x_j) = (x_i \\cdot x_j + c)^d$\n",
    "- **RBF (Radial Basis Function) or Gaussian Kernel**: $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$\n",
    "- **Sigmoid Kernel**: $K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)$\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "#### Advantages\n",
    "- Can capture complex, non-linear relationships in data.\n",
    "- Suitable for clustering, classification, and other tasks where linear methods fail.\n",
    "\n",
    "#### Limitations\n",
    "- Computationally more expensive compared to standard PCA.\n",
    "- Requires choosing an appropriate kernel function and tuning its parameters, which may not be straightforward.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data for concentric circles\n",
    "np.random.seed(0)\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Apply regular PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Apply Kernel PCA with Radial Basis Function (RBF) kernel\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=1, n_components=2)\n",
    "X_kpca = kpca.fit_transform(X_std)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot original data\n",
    "axes[0].scatter(X_std[y == 0, 0], X_std[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[0].scatter(X_std[y == 1, 0], X_std[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# Plot data transformed by PCA\n",
    "axes[1].scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[1].scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[1].set_title(\"After PCA\")\n",
    "\n",
    "# Plot data transformed by Kernel PCA\n",
    "axes[2].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[2].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[2].set_title(\"After Kernel PCA\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor analysis\n",
    "\n",
    "Factor Analysis as a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. With PCA, we generally start with no knowledge of the underlying \"meaning\" of the data - we might engage in some kind of post-hoc construction, but typically PCA is a knowledge-lean method that involves no knowledge of the domain.  Factor analysis, on the other hand, assumes that the there are underlying \"forces\" that shape the data. The goal of factor analysis is to find evidence for these forces.\n",
    "\n",
    "For example, in psychology, we might have a set of questionnaire responses that we believe reflect broader latent traits like extraversion or conscientiousness. Each question doesn't measure these traits directly, but together, they give us clues about their presence, much like how several landmarks might hint at the treasure's location. Factor analysis helps us quantify these traits and understand how each question relates to them.\n",
    "\n",
    "It may be useful to contrast Factor Analysis with regression analysis as it used in the social sciences:\n",
    "\n",
    "**Regression Analysis**:\n",
    "- **Purpose**: It's primarily used for prediction and for testing hypotheses about relationships between dependent (outcome) and independent (predictor) variables.\n",
    "- **Variables**: In regression, you have clearly defined dependent and independent variables. The independent variables are thought to influence the dependent variable directly, and they are measured without error.\n",
    "- **Assumptions**: It assumes that there is a direct causal relationship between the variables.\n",
    "- **Orientation**: It is typically predictive in nature, aiming to quantify the impact of one or more independent variables on a dependent variable.\n",
    "\n",
    "**Factor Analysis**:\n",
    "- **Purpose**: It's used for identifying latent constructs that aren't directly observed but are inferred from the relationships among observed variables.\n",
    "- **Variables**: There are no explicitly defined independent or dependent variables at the outset. Instead, you have a set of observed variables that are believed to be influenced by a smaller number of unobserved factors.\n",
    "- **Assumptions**: It assumes that the observed variables are manifestations of underlying factors. The factors themselves are what the analysis seeks to understand and quantify.\n",
    "- **Orientation**: It is exploratory (or confirmatory, in the case of CFA) rather than predictive. The goal is to understand the structure of the data and the dimensions that underlie patterns of covariation.\n",
    "\n",
    "So, in essence, while regression is about establishing a directional relationship from cause (independent variables) to effect (dependent variable), factor analysis is about uncovering the underlying structure where the 'independent variables' (factors) are not directly observed but are suggested by the patterns of correlation among the observed variables. It's more about dimension reduction and latent variable modeling than about prediction.\n",
    "\n",
    "## Steps in Factors Analysis\n",
    "\n",
    "Factor analysis is a multistep process, that involve an initial dimensionality reduction, followed by rotation of extracted factors and interpretation.\n",
    "\n",
    "In factor analysis, the dimensionality reduction technique used is based on a model where observed variables are theorized to be linear combinations of potential factors, plus \"error\" terms. Unlike PCA, which is a purely algebraic method, factor analysis is a statistical model that aims to explain the correlations among observed variables in terms of fewer unobserved variables (the factors).\n",
    "\n",
    "1. **Factor Extraction**: This is the process of estimating the number and nature of the factors that influence the set of observed variables. There are various methods for factor extraction. The choice of which can be informed by the size and nature of your dataset. For example, Maximum Likelihood Factor Analysis requires larger sample sizes to produce stable estimates and is more sensitive to violations of the normality assumption. On the other hand, Principal Axis Factoring can be used with smaller samples and is not as sensitive to non-normality.\n",
    "\n",
    "In practice, it's common to try different methods and compare the results, as different methods can lead to different factor solutions. The choice of method is often guided by theoretical considerations about the data, the robustness of different methods to violations of their assumptions, and the interpretability of the resulting solution.\n",
    "\n",
    "   - **Principal Axis Factoring (PAF)**: Similar to PCA, but it iterates to account for communalities (the proportion of each variable's variance that is accounted for by the factors). It doesn't assume that total variance is equal to 1 for each variable, unlike PCA.  This is the default in sklearn. This method is preferred if you're more interested in identifying the underlying structure or constructs that explain the correlations among observed variables. It assumes that there is a smaller set of unobserved latent variables (factors) that account for the common variance (correlation) of a set of observed variables.\n",
    "   \n",
    "   - **Maximum Likelihood Factor Analysis (MLFA)**: This method uses a likelihood function and assumes that the factors are normally distributed. It finds factor loadings that maximize the likelihood of the observed correlations under a normal distribution. Choose this method if you want to use a statistically rigorous approach that allows hypothesis testing, such as the goodness of fit of the model. This method assumes that the observed variables are multivariate normally distributed.\n",
    "\n",
    "   - **Generalized Least Squares (GLS)** and **Unweighted Least Squares (ULS)**: These methods minimize the discrepancy between the sample and the reproduced correlation matrices. These methods might be chosen if you want a simpler computational method without the distributional assumptions of ML. Least squares methods minimize the discrepancy between the observed correlation matrix and the correlation matrix predicted by the factor model.\n",
    "\n",
    "2. **Rotation**: After extracting the factors, they can be rotated to achieve a simpler and more interpretable structure. Rotations can be orthogonal (where factors remain uncorrelated) or oblique (where factors can be correlated).\n",
    "\n",
    "   - **Orthogonal rotation (e.g., Varimax)** tries to maximize the variance of squared loadings of a factor (column) on all the variables (rows) in a factor matrix, under the constraint that factors remain orthogonal.\n",
    "   \n",
    "   - **Oblique rotation (e.g., Promax)** allows for the factors to correlate and might be used when the theoretical constructs are expected to be related.\n",
    "\n",
    "3. **Factor Scores**: Once factors have been extracted and rotated, you can compute factor scores, which are estimates of the factor values for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applying Factor Analysis\n",
    "\n",
    "1. **Assess Suitability**: Before performing factor analysis, you need to assess the suitability of your data. This includes ensuring there is some correlation between your variables, as factor analysis is based on identifying relationships. Techniques like the Kaiser-Meyer-Olkin (KMO) test or looking at a correlation matrix can help determine if factor analysis is appropriate.\n",
    "\n",
    "### A positive example\n",
    "\n",
    "Note, we'll use the `factor_analyzer` package to run the KMO test, so need to install that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Load and standardize data\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "X = scale(X)  # Standardizing the data\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Note the the digits data is missing two pixels - this will cause KMO analysis to break, and result in NA's in the correlation matrix.\n",
    "# Check for constant features\n",
    "constant_columns = df.columns[df.nunique() <= 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "# Drop constant columns if any\n",
    "df = df.drop(columns=constant_columns)\n",
    "\n",
    "# Calculate the KMO values\n",
    "kmo_all, kmo_model = calculate_kmo(df)\n",
    "\n",
    "print(f\"KMO Model: {kmo_model}\")\n",
    "\n",
    "# If KMO Model is less than 0.6, data is not suitable for factor analysis.\n",
    "# As the 'digits' dataset is known to be good for factor analysis,\n",
    "# the KMO should be well above 0.6.\n",
    "\n",
    "# Correlation analysis\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plotting the correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A negative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data\n",
    "np.random.seed(42)\n",
    "X_random = np.random.rand(100, 20)  # 100 samples and 20 features\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_random = pd.DataFrame(X_random)\n",
    "\n",
    "# Calculate the KMO values\n",
    "kmo_all_random, kmo_model_random = calculate_kmo(df_random)\n",
    "\n",
    "print(f\"KMO Model (Random Data): {kmo_model_random}\")\n",
    "\n",
    "# The KMO for random data should be low, indicating unsuitability for factor analysis.\n",
    "\n",
    "# Correlation analysis for random data\n",
    "corr_matrix_random = df_random.corr()\n",
    "\n",
    "# Plotting the correlation heatmap for random data\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix_random, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix for Random Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Choosing the Number of Factors**: In addition to theoretically motivated hypotheses, you can also apply the Kaiser criterion (eigenvalues >1) or scree plot analysis. These help determine the number of underlying factors that sufficiently capture the variability in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Load the digits data\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "\n",
    "# Find which columns in X have non-zero variance (i.e., remove empty pixels)\n",
    "non_zero_variance_mask = np.var(X, axis=0) > 0\n",
    "X = X[:, non_zero_variance_mask]\n",
    "\n",
    "# Initialize FactorAnalyzer with a high number of possible factors\n",
    "fa = FactorAnalyzer(n_factors=X.shape[1], rotation=None, method='minres')\n",
    "fa.fit(X)\n",
    "\n",
    "# Get eigenvalues and check how many are greater than 1 (Kaiser criterion)\n",
    "ev, _ = fa.get_eigenvalues()\n",
    "n_factors_kaiser = sum(ev > 1)\n",
    "print(f'Eigenvalues: {ev}')\n",
    "print(f'Number of factors (Kaiser criterion): {n_factors_kaiser}')\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, X.shape[1] + 1), ev, marker='o')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.axhline(y=1, color='r', linestyle='--')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity\n",
    "from sklearn.datasets import load_digits\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load the digits dataset\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "\n",
    "# Find which columns in X have non-zero variance (i.e., remove empty pixels)\n",
    "non_zero_variance_mask = np.var(X, axis=0) > 0\n",
    "X = X[:, non_zero_variance_mask]\n",
    "\n",
    "# Check for the adequacy of the dataset\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X)\n",
    "kmo_all, kmo_model = calculate_kmo(X)\n",
    "\n",
    "# Factor Analysis\n",
    "fa = FactorAnalyzer(rotation=None, impute='drop', n_factors=X.shape[1])\n",
    "fa.fit(X)\n",
    "eigen_values, _ = fa.get_eigenvalues()\n",
    "\n",
    "# Perform Parallel Analysis\n",
    "np.random.seed(0)\n",
    "rand_data = np.random.normal(0, 1, size=X.shape)\n",
    "fa_random = FactorAnalyzer(rotation=None, impute='drop', n_factors=X.shape[1])\n",
    "fa_random.fit(rand_data)\n",
    "random_eigen_values, _ = fa_random.get_eigenvalues()\n",
    "\n",
    "# Plot actual vs random eigenvalues\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, X.shape[1]+1), eigen_values, label='Actual Data')\n",
    "plt.plot(range(1, X.shape[1]+1), random_eigen_values, label='Random Data', linestyle='--')\n",
    "plt.xlabel('Factor Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Parallel Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Running Factor Analysis:\n",
    "\n",
    "4. **Fitting the Model**: In scikit-learn, you would create a `FactorAnalysis` object and fit it to your data using the `fit` method. Under the hood, the algorithm is using a maximum likelihood approach to estimate the loadings (the relationship strength between variables and factors) and unique variances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Load the digits dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Perform factor analysis with 10 components\n",
    "fa = FactorAnalysis(n_components=10, random_state=0)\n",
    "fa.fit(X)\n",
    "\n",
    "# We'll create a figure with 2 rows and 5 columns\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Reshape the loadings to 8x8\n",
    "    loading = fa.components_[i].reshape(8, 8)\n",
    "    \n",
    "    # Create a heatmap for each loading, remove the axis labels and ticks\n",
    "    sns.heatmap(loading, ax=ax, cmap='viridis', cbar=False)\n",
    "    ax.set_title(f'Factor {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at how factors load for the different digits\n",
    "\n",
    "digit_factor_loadings = np.zeros((10, 10))\n",
    "for digit in range(10):\n",
    "    digit_data = X[y == digit]\n",
    "    digit_factors = fa.transform(digit_data)\n",
    "    digit_factor_loadings[digit, :] = np.mean(digit_factors, axis=0)\n",
    "\n",
    "# Visualize the average loadings for each digit\n",
    "g = sns.clustermap(digit_factor_loadings, col_cluster=False,method='ward', cmap='viridis')\n",
    "\n",
    "# Hide the color bar (legend)\n",
    "g.cax.set_visible(False)\n",
    "\n",
    "#g.ax_heatmap.set_position([.25, 0, 0.9, 0.9])\n",
    "g.ax_heatmap.set_xlabel('Factors')\n",
    "g.ax_heatmap.set_ylabel('Digits') \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Factor Rotation**: This is an optional step, but can be performed to achieve a simpler and more interpretable structure. It involves applying orthogonal or oblique rotations to the factor loadings. This is just a parameter to sklearn's factor analysis routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Load the digits dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Perform factor analysis with 10 components\n",
    "fa = FactorAnalysis(n_components=10, random_state=0, rotation=\"varimax\")\n",
    "fa.fit(X)\n",
    "\n",
    "# We'll create a figure with 2 rows and 5 columns\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Reshape the loadings to 8x8\n",
    "    loading = fa.components_[i].reshape(8, 8)\n",
    "    \n",
    "    # Create a heatmap for each loading, remove the axis labels and ticks\n",
    "    sns.heatmap(loading, ax=ax, cmap='viridis', cbar=False)\n",
    "    ax.set_title(f'Factor {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_factor_loadings = np.zeros((10, 10))\n",
    "for digit in range(10):\n",
    "    digit_data = X[y == digit]\n",
    "    digit_factors = fa.transform(digit_data)\n",
    "    digit_factor_loadings[digit, :] = np.mean(digit_factors, axis=0)\n",
    "\n",
    "# Visualize the average loadings for each digit\n",
    "g = sns.clustermap(digit_factor_loadings, col_cluster=False,method='ward', cmap='viridis')\n",
    "\n",
    "# Hide the color bar (legend)\n",
    "g.cax.set_visible(False)\n",
    "\n",
    "g.ax_heatmap.set_xlabel('Factors')\n",
    "g.ax_heatmap.set_ylabel('Digits') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "In addition to examining the factor loadings (above) you can also examine `communalities` and `unique variances`.  \n",
    "\n",
    "7. **Communalities**: This is the proportion of each variable's variance that is explained by the factors. A higher communality indicates that more of the variance is captured by the model.\n",
    "   - High Communalities: If a variable has a high communality, it means that most of its variance is accounted for by the extracted factors. This implies that the factor model is a good fit for this variable.\n",
    "\n",
    "   - Low Communalities: On the other hand, if a variable has a low communality, it means that the extracted factors do not explain much of its variance. This might suggest that the variable is unique or that it might be influenced by factors that are not captured by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = fa.components_.T\n",
    "communalities = np.sum(loadings**2, axis=1)\n",
    "communality_df = pd.DataFrame(communalities, columns=['Communality'])\n",
    "print(\"\\nCommunalities:\")\n",
    "print(communality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a heatmap\n",
    "communalities_matrix = communalities.reshape(8, 8)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(communalities_matrix, cmap='viridis', annot=True, fmt=\".2f\")\n",
    "plt.title('Communalities Heatmap')\n",
    "plt.xlabel('Pixel X Position')\n",
    "plt.ylabel('Pixel Y Position')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8. **Unique Variances**: Scikit-learn also provides unique variances which are variances of each variable that are not explained by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variances = fa.noise_variance_\n",
    "unique_variance_df = pd.DataFrame(unique_variances, columns=['Unique Variance'])\n",
    "\n",
    "print(\"\\nUnique Variances:\")\n",
    "print(unique_variance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a heatmap\n",
    "unique_variances_matrix = unique_variances.reshape(8, 8)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(unique_variances_matrix, cmap='viridis', annot=True, fmt=\".2f\")\n",
    "plt.title('Unique Variances Heatmap')\n",
    "plt.xlabel('Pixel X Position')\n",
    "plt.ylabel('Pixel Y Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Try using factor analysis on the `iris` data.  Compare PCA, unrotated FA, and Varimax FA, all with 2 components.  Use heatmaps to inspect the relationship between components and the different features.  What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Techniques\n",
    "\n",
    "Adaptive techniques seek to preserve complex, non-linear relationships between data points. Unlike linear methods like PCA, which apply a rigid transformation to all points in the dataset equally, adaptive methods tweak and optimize the transformation for different regions of the data space. In simpler terms, they adapt to the underlying data distribution.\n",
    "\n",
    "This adaptability often makes these techniques more suitable for capturing and preserving intricate structures in high-dimensional data, such as clusters or manifolds.  However, they are often stochastic and are not guaranteed to preserve distances in the transformed data, so may not be suitable in all cases.  They are, however, excellent for visualization and some kinds of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "t-SNE is particularly effective for visualizing high-dimensional data in 2D or 3D.\n",
    " \n",
    "#### How it Works:\n",
    "\n",
    "1. **Probability Distributions**: t-SNE starts by calculating the pairwise similarities between points in the high-dimensional space and in the low-dimensional space. These similarities are converted into conditional probabilities that a point would pick another point as its neighbor if neighbors were picked in proportion to their similarity.\n",
    "\n",
    "2. **Minimize the Divergence**: t-SNE aims to minimize the difference between these two probability distributions for the high-dimensional and low-dimensional spaces. Specifically, it minimizes the Kullback-Leibler divergence between them.\n",
    "\n",
    "3. **t-Distribution**: To calculate the similarity between points in the low-dimensional space, t-SNE uses a t-distribution, which has heavier tails compared to a normal distribution. This allows t-SNE to be particularly sensitive to local structures.\n",
    "\n",
    "#### Strengths:\n",
    "\n",
    "- Excellent at preserving local cluster structures.\n",
    "- Particularly useful for visualization.\n",
    "\n",
    "#### Weaknesses:\n",
    "\n",
    "- Computationally intensive.\n",
    "- May not preserve global structures well.\n",
    "- The results are sensitive to hyperparameters like perplexity and the learning rate.\n",
    "- **IMPORTANT** t-SNE does not enable \"training\" - you cannot fit a model a and then apply it to new data.  As a result, it is not really appopriate for train / test scenarios.\n",
    "\n",
    "t-SNE has been a popular choice for tasks like visualizing gene expression data, understanding neural network activations, clustering, and much more.\n",
    "\n",
    "With t-SNE, it's easier to capture the non-linear manifolds and groupings in data, making it an excellent tool for exploratory data analysis. However, because it's computationally demanding and somewhat unpredictable, it may not always be the best choice for all types of dimensionality reduction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "A classic example of data that PCA struggles with is the `swiss roll`, which you can envision as a sheet of paper rolled into a tube.  You'll see the PCA manages to uncover the spiral, but not it's \"sheet-like\" properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create Swiss Roll data\n",
    "n_samples = 1500\n",
    "noise = 0.05\n",
    "X, color = make_swiss_roll(n_samples, noise=noise)\n",
    "# Make it thinner\n",
    "X[:, 1] *= .5\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 3D Swiss Roll\n",
    "ax0 = fig.add_subplot(131, projection='3d')\n",
    "ax0.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax0.set_title(\"Original 3D Swiss Roll\")\n",
    "\n",
    "# PCA\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax1.set_title(\"PCA of Swiss Roll\")\n",
    "\n",
    "# t-SNE\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax2.set_title(\"t-SNE of Swiss Roll\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Try using t-SNE to visualize the MNIST data above.  How does it compare to PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try runing the KNN algorithm again.  How does the classifier perform now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "UMAP is a newer technique for dimensionality reduction and data visualization, like t-SNE. It is designed to provide more robust results and be computationally efficient, making it suitable for larger datasets.\n",
    "\n",
    "### Key Features of UMAP:\n",
    "\n",
    "1. **Preservation of Topology:** UMAP aims to preserve not just local but also global data structures. The emphasis on global / local structure can be controlled by parameters. In comparison, t-SNE excels at preserving local structures but may not always capture the global relationships as accurately.\n",
    "\n",
    "2. **Computational Efficiency:** UMAP is generally faster than t-SNE, especially for larger datasets, making it a more scalable option.\n",
    "\n",
    "3. **Versatility:** UMAP is not just for visualization; you can also use it for general non-linear dimensionality reduction tasks.\n",
    "\n",
    "4. **Compatibility with Metrics:** Unlike t-SNE, which mainly uses Euclidean distance, UMAP can work with a variety of distance metrics.\n",
    "\n",
    "### Working Principle:\n",
    "\n",
    "1. **Mathematical Foundations:** UMAP is grounded in rigorous mathematics, specifically the theory of Riemannian geometry and algebraic topology.\n",
    "\n",
    "2. **Manifold Learning:** Similar to t-SNE, UMAP also operates on the principle that the data is sampled from some low-dimensional manifold embedded in a high-dimensional space. It works to learn this manifold structure.\n",
    "\n",
    "3. **Graph-based Approach:** UMAP first constructs a weighted k-NN graph for the data in the original space. It then optimizes a similar graph in the lower-dimensional space to be as structurally similar to the original graph as possible.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Low dimensional manifolds</b>: A \"low-dimensional manifold\" refers to a manifold of lower dimensionality than the space in which it is embedded. For example, consider a two-dimensional sheet of paper that is crumpled and placed in a three-dimensional room. Despite the crumpling, each point on the paper still has a neighborhood that resembles a flat, 2D plane. Thus, the crumpled paper represents a 2D manifold within the 3D space of the room.\n",
    "\n",
    "The concept of a low-dimensional manifold is often invoked to suggest that the meaningful dimensions of the data are fewer than the number of columns in the data table. For instance, you might have a 1,000-dimensional dataset that actually lies along a curved, 10-dimensional surface within that 1,000-dimensional space. Machine learning techniques like manifold learning aim to discover this low-dimensional surface (or manifold), making the data easier to visualize, understand, and work with.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to install UMAP learn for UMAP\n",
    "# Unfortunately, UMAP learn currently depends on an older version of numpy\n",
    "# To handle this, you'll want to create a new environment, install the necessary libraries\n",
    "# and switch to this kernel for all processing\n",
    "\n",
    "!pip install numpy==1.24\n",
    "!pip install umap-learn\n",
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here's an example with the `Swiss roll` - note that both seem to do a pretty good job, but UMAP is quite a bit faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import time\n",
    "\n",
    "# Generate Swiss roll data\n",
    "n_samples = 1500\n",
    "noise = 0.05\n",
    "X, color = make_swiss_roll(n_samples, noise=noise)\n",
    "\n",
    "# Apply t-SNE\n",
    "start_time_tsne = time.time()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "end_time_tsne = time.time()\n",
    "\n",
    "# Apply UMAP\n",
    "start_time_umap = time.time()\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "end_time_umap = time.time()\n",
    "\n",
    "elapsed_tsne = end_time_tsne - start_time_tsne\n",
    "elapsed_umap = end_time_umap - start_time_umap\n",
    "print(f\"Time elapsed for t-SNE: {elapsed_tsne:.4f} seconds\")\n",
    "print(f\"Time elapsed for UMAP: {elapsed_umap:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original Swiss roll in 3D\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax1.set_title('Original Swiss Roll in 3D')\n",
    "ax1.set_xlabel('X-axis')\n",
    "ax1.set_ylabel('Y-axis')\n",
    "ax1.set_zlabel('Z-axis')\n",
    "\n",
    "# t-SNE plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax2.set_title('t-SNE')\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "\n",
    "# UMAP plot\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(X_umap[:, 0], X_umap[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax3.set_title('UMAP')\n",
    "ax3.set_xlabel('UMAP 1')\n",
    "ax3.set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding umap parameters\n",
    "\n",
    "There is a wonderful [online tutorial](https://pair-code.github.io/understanding-umap/) that illustrates the impact of UMAP parameters.  I've replicated part of it here for you.  It uses data from a 3D scan of a Wooley Mammoth, curated by [Maximilian Noichl](https://homepage.univie.ac.at/maximilian.noichl/post/mammoth/).\n",
    "\n",
    "The basic idea here is that the `nearest neighbors` parameter adjust sensitivity to global structure, and `min-dist` parameter controls the \"fuzziness\" of the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here's the 3D mammoth\n",
    "import pandas as pd\n",
    "mammoth = pd.read_csv('./data/mammoth_a.csv')\n",
    "mammoth = mammoth.sample(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(90,72))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax.scatter(mammoth['x'], mammoth['y'], mammoth['z'], s=20,c='black')\n",
    "ax.view_init(20, -170)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings take a while to process, especially when the number of neighbors increases, but here's an example with a relatively low number of nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "umap = umap.UMAP(    init='random',\n",
    "                    n_components=2,\n",
    "                    n_neighbors=30,\n",
    "                    min_dist=0.1,\n",
    "                     spread=2,\n",
    "                    metric='euclidean',\n",
    "                    verbose=True)\n",
    "\n",
    "mammoth_transformed = umap.fit(mammoth.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that UMAP has two methods, `fit` and `fit_transform`, like most scikit learn ML models.  With the `fit` method, UMAP will create a model that can then be applied to new data.  This is a kind of supervised learning, allowing you to build an embedding model on a small bit of data first, and then extending it larger data.  See the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/basic_usage.html#digits-data) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20),facecolor='w')\n",
    "plt.axis('off')\n",
    "plt.scatter(mammoth_transformed.embedding_[:, 0], mammoth_transformed.embedding_[:, 1], s=1,color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Maximilian Noichl](https://homepage.univie.ac.at/maximilian.noichl/post/mammoth/) generated a few animations that help illustrate how UMAP behaves across different parameter settings.\n",
    "\n",
    "#### Exploring Min Dist\n",
    "\n",
    "![Min dist animation](./assets/mammoth_md.gif)\n",
    "\n",
    "#### Exploring Nearest Neighbors\n",
    "\n",
    "![Min dist animation](./assets/mammoth_nn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Try using UMAP to process the MNIST data.  Visualize the data first, and then calculate accuracy with KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here - just use default parameters for UMAP\n",
    "import umap\n",
    "# Capture your data data in the following variable\n",
    "X_umap = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y_mnist_train, cmap=\"Spectral\",s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title(\"MNIST - 2D UMAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Try exploring the impact of dimensionality reduction using the following sample data.\n",
    "\n",
    "1. How does a KNN classifier do as the number of dimensions increases?\n",
    "2. Apply a PCA that captures 90% of the variance. Does it help?\n",
    "3. Does UMAP help?\n",
    "4. What explains the difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_points = 500\n",
    "\n",
    "# Class 0\n",
    "x0 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "y0 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "z0 = np.random.normal(loc=0, scale=1, size=n_points)\n",
    "\n",
    "# Class 1\n",
    "x1 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "y1 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "z1 = np.random.normal(loc=2, scale=1, size=n_points)\n",
    "\n",
    "X = np.vstack((np.hstack((x0, x1)), np.hstack((y0, y1)), np.hstack((z0, z1)))).T\n",
    "y = np.hstack((np.zeros(n_points), np.ones(n_points)))\n",
    "\n",
    "\n",
    "\n",
    "# Train KNN on original data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "print(f\"Original Data Accuracy: {accuracy_score(y, y_pred)}\")\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "# Add 3D subplot\n",
    "# The format is (rows, columns, plot_number)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=0, azim=30)\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis')\n",
    "ax.set_title('Original Data')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
