{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the following data:\n",
    "\n",
    "| Tid | Refund | Marital Status | Taxable Income (K) | Cheat |\n",
    "|-----|--------|----------------|--------------------|-------|\n",
    "| 1   | Yes    | Single         | 125                | No    |\n",
    "| 2   | No     | Married        | 100                | No    |\n",
    "| 3   | No     | Single         | 70                 | No    |\n",
    "| 4   | Yes    | Married        | 120                | No    |\n",
    "| 5   | No     | Divorced       | 95                 | Yes   |\n",
    "| 6   | No     | Married        | 60                 | No    |\n",
    "| 7   | Yes    | Divorced       | 220                | No    |\n",
    "| 8   | No     | Single         | 85                 | Yes   |\n",
    "| 9   | No     | Married        | 75                 | No    |\n",
    "| 10  | No     | Single         | 90                 | Yes   |\n",
    "\n",
    "Complete the code below to find the best split.  Note that I've written most of the code for you - all you need to do is implement the `gini_impurity` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Split: Column Marital Status with value Married gini score = 0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data provided\n",
    "data = {\n",
    "    'Tid': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Refund': ['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No'],\n",
    "    'Marital Status': ['Single', 'Married', 'Single', 'Married', 'Divorced', 'Married', 'Divorced', 'Single', 'Married', 'Single'],\n",
    "    'Taxable Income (K)': [125, 100, 70, 120, 95, 60, 220, 85, 75, 90],\n",
    "    'Cheat': ['No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate Gini impurity\n",
    "def gini_impurity(groups, classes):\n",
    "    # IMPLEMENT ME!!!\n",
    "    gini = None\n",
    "    return gini\n",
    "\n",
    "# Function to split a dataset based on a column and a value\n",
    "def test_split(column, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        # If column is numeric, split by <= value or > value\n",
    "        if isinstance(value, (int, float)):\n",
    "            if row[column] <= value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        # If column is categorical, split by == value\n",
    "        else:\n",
    "            if row[column] == value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Function to get quartiles for continuous variables\n",
    "def get_quartiles(series):\n",
    "    return np.percentile(series, [25, 50, 75])\n",
    "\n",
    "# Function to find the best split\n",
    "def get_best_split(df):\n",
    "    dataset = df.values.tolist()  # Convert dataframe to list of lists\n",
    "    class_values = list(df['Cheat'].unique())  # Unique class labels\n",
    "    best_column, best_value, best_gini, best_groups = None, None, float('inf'), None\n",
    "    \n",
    "    # Iterate over each column (feature)\n",
    "    for column in df.columns[:-1]:  # Exclude the target variable 'Cheat'\n",
    "        if df[column].dtype == 'object':  # Categorical column\n",
    "            unique_values = df[column].unique()\n",
    "            for value in unique_values:\n",
    "                groups = test_split(df.columns.get_loc(column), value, dataset)\n",
    "                gini = gini_impurity(groups, class_values)\n",
    "                if gini < best_gini:\n",
    "                    best_column, best_value, best_gini, best_groups = column, value, gini, groups\n",
    "        \n",
    "        elif np.issubdtype(df[column].dtype, np.number):  # Numeric column\n",
    "            quartiles = get_quartiles(df[column])\n",
    "            for value in quartiles:  # Try splitting by quartiles\n",
    "                groups = test_split(df.columns.get_loc(column), value, dataset)\n",
    "                gini = gini_impurity(groups, class_values)\n",
    "                if gini < best_gini:\n",
    "                    best_column, best_value, best_gini, best_groups = column, value, gini, groups\n",
    "    \n",
    "    return {'column': best_column, 'value': best_value, 'gini': best_gini, 'groups': best_groups}\n",
    "\n",
    "# Call the function to find the best split\n",
    "best_split = get_best_split(df)\n",
    "print('Best Split: Column', best_split['column'], 'with value', best_split['value'], 'gini score =', best_split['gini'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Build a decision tree to fit the [federalist papers](https://www.kaggle.com/datasets/tobyanderson/federalist-papers) data, available in the data directory (click on the link to find out more information about this data). Note that you should restrict your analysis to papers by Hamilton or Madison.  Plot your training and test scores to pick a value for ccp_alpha. What did you pick?  Run your trained classifier on the \"disputed\" papers.  What does your model tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Build a voting classifier for the federalist papers, using all of the non-ensemble methods you've been exposed to in this class thus far (i.e., KNN, SVM, logistic regression, naive bayes, SGDClassifier, decision tree).\n",
    "\n",
    "1) Compare this to a RandomForest classifier.  Which works the best?\n",
    "2) Compare this to a GradientBoosting classifier.  Which works the best?\n",
    "3) Add the RandomForest and GradientBoosting classifiers to your voting classifier.  Does you performance improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "When does it make sense to use a Bagging Classifier?  In the following, explore different data parameters to develop your intuition for which classifier makes sense in which situation. \n",
    "\n",
    "1. Gradually increase the noise in the data (using the noise parameter).  How do the different classifiers perform.  Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_train_pred_tree = tree.predict(X_train)\n",
    "print(\"Decision Tree Accuracy (train):\", accuracy_score(y_train, y_train_pred_tree))\n",
    "print(\"Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "y_train_pred_log = log_reg.predict(X_train)\n",
    "print(\"Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_train_pred_log))\n",
    "print(\"Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Now, do the same thing in the following.  What do you notice. How do you explain your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_samples = 100\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_tree = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_tree.fit(X_train, y_train)\n",
    "y_pred_bag_train = bag_tree.predict(X_train)\n",
    "y_pred_bag = bag_tree.predict(X_test)\n",
    "print(\"Bagging Decision Tree Accuracy (train):\", accuracy_score(y_train, y_pred_bag_train))\n",
    "print(\"Bagging Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_bag))\n",
    "\n",
    "\n",
    "bag_log = BaggingClassifier(LogisticRegression(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_log.fit(X_train, y_train)\n",
    "y_pred_bag_log_train = bag_log.predict(X_train)\n",
    "y_pred_bag_log = bag_log.predict(X_test)\n",
    "print(\"Bagging Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_pred_bag_log_train))\n",
    "print(\"Bagging Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_bag_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now go back and start increasing the `max_samples` parameter.  How do things change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "The \"wine\" dataset contains data about the chemical makeup of different varieties of wine and critics scores.  Use XGBoost to build a classifier for this data.  Manually tune the hyperparameters of the XGBoost model to try to achieve better accuracy on the test set than the baseline model. Some hyperparameters to consider tweaking:\n",
    "   - `learning_rate`\n",
    "   - `max_depth`\n",
    "   - `n_estimators`\n",
    "   - `gamma`\n",
    "   - `subsample`\n",
    "   - `colsample_bytree`\n",
    "\n",
    "See [the online docs](https://xgboost.readthedocs.io/en/stable/parameter.html) for more info.\n",
    "\n",
    "After tuning, use the `plot_importance` function again to see if feature importances have changed after tuning.\n",
    "\n",
    "\n",
    "1. How did hyperparameter tuning affect the model's accuracy? Which hyperparameters seemed to have the most influence?\n",
    "2. Did feature importances change after tuning? If so, why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you don't have XGBoost installed\n",
    "%pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "data = load_wine()\n",
    "\n",
    "# We'll use a data frame to make sure we get real feature names out\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='multi:softprob', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "plot_importance(clf)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
